{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2e6f6927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, copy, shutil\n",
    "import csv, json\n",
    "import random, numpy as np\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e1fc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy all files from `unbroken_data` to `broken_data`.\n",
    "\n",
    "if os.path.exists('data'):\n",
    "    shutil.rmtree('data')\n",
    "os.mkdir('data')\n",
    "\n",
    "for file in os.listdir('unbroken_data'):\n",
    "    shutil.copy(os.path.join('unbroken_data', file), os.path.join('data', file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b972bab4",
   "metadata": {},
   "source": [
    "#### Break `planets_4.csv`:\n",
    "\n",
    "- All changes are only made to either the first ten or last ten rows so they can be easily identified.\n",
    "- Each broken row is of the following form:\n",
    "    - Some columns are deleted, so rows have less length - causes `IndexError`,\n",
    "    - `Discovery Method` (**str** type) column is swapped with some numeric column - causes `ValueError`,\n",
    "    - `Planet Name` column is changed to junk values - causes `KeyError`,\n",
    "    - Entire row comes from a different `planets` file, but `mapping` does not contain this Planet-Star - causes `KeyError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829c449e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Planet Name',\n",
       "  'Discovery Method',\n",
       "  'Discovery Year',\n",
       "  'Controversial Flag',\n",
       "  'Orbital Period [days]',\n",
       "  'Planet Radius [Earth Radius]',\n",
       "  'Planet Mass [Earth Mass]',\n",
       "  'Orbit Semi-Major Axis [au]',\n",
       "  'Eccentricity',\n",
       "  'Equilibrium Temperature [K]',\n",
       "  'Insolation Flux [Earth Flux]'],\n",
       " ['2MASS J19383260+4603591 b',\n",
       "  'Eclipse Timing Variations',\n",
       "  '2015',\n",
       "  '0',\n",
       "  '406.00000000',\n",
       "  '13.400',\n",
       "  '603.87700',\n",
       "  '0.920000',\n",
       "  '0.330000',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read all the `planets` data\n",
    "\n",
    "planets = {}\n",
    "for i in range(1, 6):\n",
    "    f = open(os.path.join('unbroken_data', 'planets_%d.csv' % (i)), encoding='utf-8')\n",
    "    planets[i] = list(csv.reader(f))\n",
    "    f.close()\n",
    "\n",
    "planets_header = planets[1][0]\n",
    "planets[4][:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb62529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_error_1(row):\n",
    "    '''some colum is deleted, so row has less length'''\n",
    "    row = copy.copy(row)\n",
    "    row.pop(random.randint(0, len(row)-1))\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e8520d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_error_2(row):\n",
    "    '''`Discovery Method` column is swapped with some numeric column'''\n",
    "    row = copy.copy(row)\n",
    "    disc_i = planets_header.index('Discovery Method')\n",
    "    rand_i = random.randint(4, len(row)-1)\n",
    "    row[disc_i], row[rand_i] = row[rand_i], row[disc_i]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d794517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_error_3(row):\n",
    "    '''`Planet Name` column is changed to some junk value'''\n",
    "    row = copy.copy(row)\n",
    "    name_i = planets_header.index('Planet Name')\n",
    "    rand_i = random.randint(1, len(row)-1)\n",
    "    row[name_i] = row[rand_i]\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2efb78bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def introduce_error_4():\n",
    "    '''new row is returned from a different planets file'''\n",
    "    file = random.choice([1, 2, 3, 5])\n",
    "    row = random.randint(1, len(planets[file])-1)\n",
    "    return planets[file][row]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76fc4886",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_modify = list(range(1, 11)) + list(range(-1, -11, -1)) # identify first ten and last ten row idxs\n",
    "rows_to_modify = list(np.random.permutation(rows_to_modify))[:10] # randomly pick any ten out of those\n",
    "new_planets = [planets_header] + copy.deepcopy(planets[4]) # make new dataset to break data rows in\n",
    "\n",
    "for row in rows_to_modify:\n",
    "    error_choice = random.randint(1, 6) # randomly pick which error to introduce\n",
    "    if error_choice in [1, 2]: # introduce IndexError\n",
    "        new_planets[row] = introduce_error_1(planets[4][row])\n",
    "    elif error_choice in [3, 4]: # introduce ValueError\n",
    "        new_planets[row] = introduce_error_2(planets[4][row])\n",
    "    elif error_choice in [5]: # introduce KeyError\n",
    "        new_planets[row] = introduce_error_3(planets[4][row])\n",
    "    elif error_choice in [6]: # introduce KeyError\n",
    "        new_planets.insert(row, introduce_error_4())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "97e15fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset with broken rows on top of `planets_4.csv`\n",
    "\n",
    "with open(os.path.join('data', 'planets_4.csv'), 'w', encoding='utf-8', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(new_planets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e90d865",
   "metadata": {},
   "source": [
    "#### Break `mapping_5.json`:\n",
    "\n",
    "- At random places, insert `:`, `}`, `\"`, `,` symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21315bc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"55 Cnc b\":\"55 Cnc\",\"55 Cnc c\":\"55 Cnc\",\"55 Cnc d'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the `mapping_5.json` data\n",
    "\n",
    "f = open(os.path.join('unbroken_data', 'mapping_5.json'), encoding='utf-8')\n",
    "mapping = f.read()\n",
    "f.close()\n",
    "mapping[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc3c125c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_mapping = copy.copy(mapping) # make new dataset to break file in\n",
    "\n",
    "for insertions in range(10): # find 10 locations to insert random characters\n",
    "    loc = random.randint(len(new_mapping)//2, len(new_mapping)-1) # randomly pick a location\n",
    "    new_insert = random.choice([\":\", \"}\", '\"', \",\"]) # randomly pick which character to insert\n",
    "    if new_insert != '\"': # if character is not `\"`, ensure it is not wasted inside a string\n",
    "        loc = loc + new_mapping[loc:].find(',')\n",
    "    new_mapping = new_mapping[:loc] + new_insert + new_mapping[loc:] # insert the character inside the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4cc5048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# write dataset with broken rows on top of `mapping_5.json`\n",
    "\n",
    "f = open(os.path.join('data', 'mapping_5.json'), 'w', encoding='utf-8')\n",
    "f.write(new_mapping)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0458c516",
   "metadata": {},
   "source": [
    "#### Create `broken_data`:\n",
    "\n",
    "- Read the data in the broken JSON file (`mapping_5.json`) and break it down into multiple files.\n",
    "- Split up the file into several different files based on the `Host Name`,\n",
    "- Store the files in different directories under various levels of nesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05dab0a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read the unbroken version of the broken JSON file\n",
    "\n",
    "f = open(os.path.join(\"unbroken_data\", \"mapping_5.json\"), encoding=\"utf-8\")\n",
    "unbroken_data = json.load(f)\n",
    "f.close()\n",
    "len(unbroken_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c77b8f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keplers': 291, 'hds': 83, 'k2s': 40, 'tois': 32, 'gjs': 17, 'others': 57}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broken_data = {}\n",
    "broken_data['keplers'] = {}\n",
    "broken_data['hds'] = {}\n",
    "broken_data['k2s'] = {}\n",
    "broken_data['tois'] = {}\n",
    "broken_data['gjs'] = {}\n",
    "broken_data['others'] = {}\n",
    "\n",
    "for planet in unbroken_data:\n",
    "    if 'Kepler' in planet:\n",
    "        broken_data['keplers'][planet] = unbroken_data[planet]\n",
    "    elif 'HD' in planet:\n",
    "        broken_data['hds'][planet] = unbroken_data[planet]\n",
    "    elif 'K2' in planet:\n",
    "        broken_data['k2s'][planet] = unbroken_data[planet]\n",
    "    elif 'TOI' in planet:\n",
    "        broken_data['tois'][planet] = unbroken_data[planet]\n",
    "    elif 'GJ' in planet:\n",
    "        broken_data['gjs'][planet] = unbroken_data[planet]\n",
    "    else:\n",
    "        broken_data['others'][planet] = unbroken_data[planet]\n",
    "        \n",
    "{key: len(broken_data[key]) for key in broken_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd97f98a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10s': 90, '100s': 189, 'others': 12}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keplers = {}\n",
    "keplers['10s'] = {}\n",
    "keplers['100s'] = {}\n",
    "keplers['others'] = {}\n",
    "\n",
    "for planet in broken_data['keplers']:\n",
    "    num = int(planet.split(\"-\")[1].split()[0])\n",
    "    if num < 100:\n",
    "        keplers['10s'][planet] = unbroken_data[planet]\n",
    "    elif num < 1000:\n",
    "        keplers['100s'][planet] = unbroken_data[planet]\n",
    "    else:\n",
    "        keplers['others'][planet] = unbroken_data[planet]\n",
    "broken_data['keplers'] = keplers\n",
    "        \n",
    "{key: len(broken_data['keplers'][key]) for key in broken_data['keplers']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "992cdea3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'10000s': 34, 'others': 49}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hds = {}\n",
    "hds['10000s'] = {}\n",
    "hds['others'] = {}\n",
    "\n",
    "for planet in broken_data['hds']:\n",
    "    num = int(planet.split()[1])\n",
    "    if num < 100000:\n",
    "        hds['10000s'][planet] = unbroken_data[planet]\n",
    "    else:\n",
    "        hds['others'][planet] = unbroken_data[planet]\n",
    "broken_data['hds'] = hds\n",
    "        \n",
    "{key: len(broken_data['hds'][key]) for key in broken_data['hds']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6bd845a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'20s': 14, '30s': 14, '80s': 20, 'others': 42}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_10s = {}\n",
    "kepler_10s['20s'] = {}\n",
    "kepler_10s['30s'] = {}\n",
    "kepler_10s['80s'] = {}\n",
    "kepler_10s['others'] = {}\n",
    "for planet in broken_data['keplers']['10s']:\n",
    "    num = int(planet.split(\"-\")[1].split()[0])\n",
    "    if 20 <= num < 30:\n",
    "        kepler_10s['20s'][planet] = unbroken_data[planet]\n",
    "    elif 30 <= num < 40:\n",
    "        kepler_10s['30s'][planet] = unbroken_data[planet]\n",
    "    elif 80 <= num < 90:\n",
    "        kepler_10s['80s'][planet] = unbroken_data[planet]\n",
    "    else:\n",
    "        kepler_10s['others'][planet] = unbroken_data[planet]\n",
    "broken_data['keplers']['10s'] = kepler_10s\n",
    "        \n",
    "{key: len(broken_data['keplers']['10s'][key]) for key in broken_data['keplers']['10s']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7308db83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100s': 74, '200s': 71, 'others': 44}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_100s = {}\n",
    "kepler_100s['100s'] = {}\n",
    "kepler_100s['200s'] = {}\n",
    "kepler_100s['others'] = {}\n",
    "\n",
    "for planet in broken_data['keplers']['100s']:\n",
    "    num = int(planet.split(\"-\")[1].split()[0])\n",
    "    if 100 <= num < 200:\n",
    "        kepler_100s['100s'][planet] = unbroken_data[planet]\n",
    "    elif 200 <= num < 300:\n",
    "        kepler_100s['200s'] [planet]= unbroken_data[planet]\n",
    "    else:\n",
    "        kepler_100s['others'][planet] = unbroken_data[planet]\n",
    "broken_data['keplers']['100s'] = kepler_100s\n",
    "        \n",
    "{key: len(broken_data['keplers']['100s'][key]) for key in broken_data['keplers']['100s']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d7214312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'220s': 16, '290s': 14, 'others': 41}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_100_200s = {}\n",
    "kepler_100_200s['220s'] = {}\n",
    "kepler_100_200s['290s'] = {}\n",
    "kepler_100_200s['others'] = {}\n",
    "\n",
    "for planet in broken_data['keplers']['100s']['200s']:\n",
    "    num = int(planet.split(\"-\")[1].split()[0])\n",
    "    if 220 <= num < 230:\n",
    "        kepler_100_200s['220s'][planet] = unbroken_data[planet]\n",
    "    elif 290 <= num < 300:\n",
    "        kepler_100_200s['290s'][planet] = unbroken_data[planet]\n",
    "    else:\n",
    "        kepler_100_200s['others'][planet] = unbroken_data[planet]\n",
    "broken_data['keplers']['100s']['200s'] = kepler_100_200s\n",
    "        \n",
    "{key: len(broken_data['keplers']['100s']['200s'][key]) for key in broken_data['keplers']['100s']['200s']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be4edfd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'100s': 17, 'others': 57}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kepler_100_100s = {}\n",
    "kepler_100_100s['100s'] = {}\n",
    "kepler_100_100s['others'] = {}\n",
    "\n",
    "for planet in broken_data['keplers']['100s']['100s']:\n",
    "    num = int(planet.split(\"-\")[1].split()[0])\n",
    "    if 100 <= num < 110:\n",
    "        kepler_100_100s['100s'][planet] = unbroken_data[planet]\n",
    "    else:\n",
    "        kepler_100_100s['others'][planet] = unbroken_data[planet]\n",
    "broken_data['keplers']['100s']['100s'] = kepler_100_100s\n",
    "        \n",
    "{key: len(broken_data['keplers']['100s']['100s'][key]) for key in broken_data['keplers']['100s']['100s']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "519d4413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomly_indent_files(broken_data):\n",
    "    '''at the last level of indentation of the file structure, sometimes randomly add another level'''\n",
    "    for file in broken_data:\n",
    "        val = broken_data[file]\n",
    "        if isinstance(val, dict) and list(val.keys())[0] in unbroken_data:\n",
    "            if random.randint(1, 3) == 1:\n",
    "                continue\n",
    "            broken_data[file] = {file: broken_data[file]}\n",
    "        else:\n",
    "            broken_data[file] = randomly_indent_files(val)\n",
    "    return broken_data\n",
    "\n",
    "broken_data = randomly_indent_files(copy.deepcopy(broken_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e485bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'keplers': {'10s': {'20s': './kepler/10/20s.json',\n",
       "   '30s': {'30s': './kepler/10/30/30s.json'},\n",
       "   '80s': './kepler/10/80s.json',\n",
       "   'others': './kepler/10/others.json'},\n",
       "  '100s': {'100s': {'100s': './kepler/100/100/100s.json',\n",
       "    'others': './kepler/100/100/others.json'},\n",
       "   '200s': {'220s': './kepler/100/200/220s.json',\n",
       "    '290s': {'290s': './kepler/100/200/290/290s.json'},\n",
       "    'others': './kepler/100/200/others.json'},\n",
       "   'others': {'others': './kepler/100/other/others.json'}},\n",
       "  'others': {'others': './kepler/other/others.json'}},\n",
       " 'hds': {'10000s': {'10000s': './hd/10000/10000s.json'},\n",
       "  'others': {'others': './hd/other/others.json'}},\n",
       " 'k2s': './k2s.json',\n",
       " 'tois': {'tois': './toi/tois.json'},\n",
       " 'gjs': {'gjs': './gj/gjs.json'},\n",
       " 'others': './others.json'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_file_structure(broken_data, unbroken_data=unbroken_data, directory=\".\"):\n",
    "    '''generate the file locations where each set of data from `broken_data` must be written'''\n",
    "    file_structure = {}\n",
    "    for file in broken_data:\n",
    "        val = broken_data[file]\n",
    "        if isinstance(val, dict) and list(val.keys())[0] in unbroken_data:\n",
    "            file_structure[file] = os.path.join(directory, file+\".json\")\n",
    "        else:\n",
    "            path = os.path.join(directory, file.rstrip('s'))\n",
    "            file_structure[file] = generate_file_structure(broken_data[file], unbroken_data, path)\n",
    "    return file_structure\n",
    "\n",
    "file_structure = generate_file_structure(broken_data)\n",
    "file_structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aee7c9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_files(file_structure=file_structure, broken_data=broken_data, directory=\".\"):\n",
    "    '''write the data in `broken_data` at the locations in `file_structure`'''\n",
    "    for file in file_structure:\n",
    "        if not isinstance(file_structure[file], dict):\n",
    "            f = open(os.path.join(directory, file+\".json\"), 'w', encoding='utf-8')\n",
    "            json.dump(broken_data[file], f)\n",
    "            f.close()\n",
    "        else:\n",
    "            path = os.path.join(directory, file.rstrip('s'))\n",
    "            os.mkdir(path)\n",
    "            generate_files(file_structure[file], broken_data[file], path)\n",
    "            \n",
    "if os.path.exists('broken_data'):\n",
    "    shutil.rmtree('broken_data')\n",
    "os.mkdir('broken_data')\n",
    "generate_files(directory='broken_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040440c9",
   "metadata": {},
   "source": [
    "#### Add files starting with `\".\"` to `data` to `broken_data`\n",
    "\n",
    "- Randomly add junk files and directories that start with `\".\"` to `data` and subdirectories of `broken_data`.\n",
    "- Zip up the directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2c040616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dot_file(directory):\n",
    "    '''create empty junk file at `directory`'''\n",
    "    if os.path.exists(os.path.join(directory, \".DS_Store\")):\n",
    "        return\n",
    "    f = open(os.path.join(directory, \".DS_Store\"), 'w')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74ceed1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dot_directory(directory):\n",
    "    '''create empty junk directory at `directory`'''\n",
    "    if os.path.exists(os.path.join(directory, \".ipynb_checkpoints\")):\n",
    "        return\n",
    "    os.mkdir(os.path.join(directory, \".ipynb_checkpoints\"))\n",
    "    f = open(os.path.join(directory,  \".ipynb_checkpoints\", \"%s-checkpoint.ipynb\" % (os.path.basename(directory))), 'w')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7a29eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_broken_directories = []\n",
    "for directory in list(os.walk('broken_data')):\n",
    "    if random.randint(1, 2) == 1: # randomly select some subdirectories of `broken_data`\n",
    "        random_broken_directories.append(directory[0])\n",
    "        \n",
    "for directory in random_broken_directories:\n",
    "    if random.randint(1, 3) != 1: # randomly add junk file to `directory` in `random_broken_directories`\n",
    "        create_dot_file(directory)\n",
    "    if random.randint(1, 3) != 1: # randomly add junk directory to `directory` in `random_broken_directories`\n",
    "        create_dot_directory(directory)\n",
    "        \n",
    "create_dot_file('data') # add junk file to `data`\n",
    "create_dot_directory('data') # add junk directory to `data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "efa88a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zip_up(directory):\n",
    "    '''zips a directory'''\n",
    "    rootlen = len(directory) + 1\n",
    "    with zipfile.ZipFile(directory + '.zip', 'w', zipfile.ZIP_DEFLATED) as zip_data:\n",
    "        for base, dirs, files in os.walk(directory):\n",
    "            for file in files:\n",
    "                file_path = os.path.join(base, file)\n",
    "                zip_data.write(file_path, file_path[rootlen:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "47835e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_up('data') # generate `data.zip`\n",
    "zip_up('broken_data') # generate `broken_data.zip`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8a57db72",
   "metadata": {},
   "outputs": [],
   "source": [
    "shutil.rmtree('data') # delete the `data` directory\n",
    "shutil.rmtree('broken_data') # delete the `broken_data` directory"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
