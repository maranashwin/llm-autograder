{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1b2de0",
   "metadata": {},
   "source": [
    "# Hidden Tests\n",
    "\n",
    "In this file, the hidden tests for all the rubric points are to be described. The tests for the individual rubric points are enclosed within `# BEGIN <rubric_point>` and `# END <rubric_point>` NBConvert cells. `hidden_tests.py` works by executing the contents of those cells between those two tags for each `<rubric_point>`. In order to initialize variables, `hidden_tests.py` also executes all code within `BEGIN` and `END` tags that appear before the `original` test.\n",
    "\n",
    "Code that is not enclosed within `BEGIN` and `END` tags are not executed by `hidden_tests.py`. They are used for generating the hidden datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e8dd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidden_tests import *\n",
    "import otter_tests.gen_public_tests as gen_public_tests\n",
    "import os, csv, json, copy, shutil\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef81058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = '..'\n",
    "FILE = 'p4.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f614845",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b15ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deductions = {}\n",
    "rubric = parse_rubric_file(os.path.join(DIRECTORY, \"rubric.md\"))\n",
    "directories = get_directories(rubric)\n",
    "comments = get_all_comments(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efe76361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_readme(data, write_path):\n",
    "    \"\"\"write_readme(data, write_path) writes the contents of `data` into the README.txt file `write_path`\"\"\"\n",
    "    f = open(write_path, encoding='utf-8')\n",
    "    rubric_point = f.read().split(\"\\n\")[0].strip(\" \\n\")\n",
    "    f.close()\n",
    "    \n",
    "    f = open(write_path, 'w', encoding='utf-8')\n",
    "    f.write(rubric_point + \"\\n\\n\" + data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb6362",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Useful variables that are used by many rubric tests can be stored here. The contents of this tag will be executed before each rubric test, so these variables get initialized before each rubric test."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9d7477c",
   "metadata": {},
   "source": [
    "# BEGIN variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eafaee",
   "metadata": {},
   "source": [
    "`verify_fn_defn` defines the function `verify_fn` which is used for verifying if the function `expected` and `actual` have the same outputs for all permutations of inputs from `var_lists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "475a0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_fn_defn = \"\"\"\n",
    "def verify_fn(expected, actual, var_inputs, test_format):\n",
    "    for var in var_inputs:\n",
    "        try:\n",
    "            actual_val = actual(*var)\n",
    "        except Exception as e:\n",
    "            output = \"%s results: \" % actual.__name__\n",
    "            output += \"%s error enountered on %s%s\" % (type(e).__name__, actual.__name__, repr(var))\n",
    "            return output\n",
    "        expected_val = expected(*var)\n",
    "        check = public_tests.compare(expected_val, actual_val, test_format)\n",
    "        if check != public_tests.PASS:\n",
    "            output = \"%s results: \" % actual.__name__\n",
    "            output += \"%s%s output: %s\" % (actual.__name__, repr(var), check)\n",
    "            return output\n",
    "    return \"%s results: All test cases passed!\" % actual.__name__\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689911b",
   "metadata": {},
   "source": [
    "`function_dependencies_functions` stores the previously defined functions that each function definition invokes. This variable is used for rubric points that the logical correctness of functions as well as those that check whether a required function is used. For these rubric points, when we test a particular function, we use `function_dependencies_functions` to ensure that all the functions that it depends on are replaced with logically correct versions. This helps isolate the issue with the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "307ebda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_dependencies_functions = {}\n",
    "function_dependencies_functions['damage'] = []\n",
    "function_dependencies_functions['type_bonus'] = []\n",
    "function_dependencies_functions['get_num_types'] = []\n",
    "function_dependencies_functions['effective_damage'] = ['get_num_types', 'type_bonus', 'damage']\n",
    "function_dependencies_functions['num_hits'] = ['damage', 'get_num_types', 'type_bonus', 'effective_damage']\n",
    "function_dependencies_functions['battle'] = ['num_hits', 'get_num_types', 'damage', 'type_bonus', 'effective_damage']\n",
    "function_dependencies_functions['friendship_score'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153663c",
   "metadata": {},
   "source": [
    "`function_dependencies_data_structures` stores the previously defined data structures that each function definition invokes. This variable is used for rubric points that the logical correctness of functions as well as those that check whether a required function is used. For these rubric points, when we test a particular function, we use `function_dependencies_data_structures` to ensure that all the data structures that it depends on are replaced with logically correct versions. This helps isolate the issue with the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "935aaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_dependencies_data_structures = {}\n",
    "function_dependencies_data_structures['damage'] = []\n",
    "function_dependencies_data_structures['type_bonus'] = []\n",
    "function_dependencies_data_structures['get_num_types'] = []\n",
    "function_dependencies_data_structures['effective_damage'] = []\n",
    "function_dependencies_data_structures['num_hits'] = []\n",
    "function_dependencies_data_structures['battle'] = []\n",
    "function_dependencies_data_structures['friendship_score'] = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bed966",
   "metadata": {},
   "source": [
    "`data_structure_dependencies_functions` stores the previously defined functions that each data structure definition invokes. This variable is used for rubric points that the logical correctness of functions as well as those that check whether a required data structure is used. For these rubric points, when we test a particular data structure, we use `data_structure_dependencies_functions` to ensure that all the functions that it depends on are replaced with logically correct versions. This helps isolate the issue with the data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e015b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_dependencies_functions = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bdf98",
   "metadata": {},
   "source": [
    "`data_structure_dependencies_data_structures` stores the previously defined data structures that each data structure definition accesses. This variable is used for rubric points that the logical correctness of data structures as well as those that check whether a required data structure is used. For these rubric points, when we test a particular data structure, we use `data_structure_dependencies_data_structures` to ensure that all the data structures that it depends on are replaced with logically correct versions. This helps isolate the issue with the data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3db1a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_dependencies_data_structures = {} "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fc84609",
   "metadata": {},
   "source": [
    "# END variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9aa19",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Useful functions that are used by many rubric tests can be stored here. The contents of this tag will be executed before each rubric test, so these function definitions get initialized before each rubric test."
   ]
  },
  {
   "cell_type": "raw",
   "id": "94d33069",
   "metadata": {},
   "source": [
    "# BEGIN functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc1d3f",
   "metadata": {},
   "source": [
    "`replace_with_false_function` replaces the given `function` with the **false version** of the function, and also replaces all **dependent** functions and data structures with their **true versions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a16ad9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_false_function(nb, function, false_function):\n",
    "    nb = replace_defn(nb, function, false_function)\n",
    "    \n",
    "    for dependent in function_dependencies_functions.get(function, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in function_dependencies_data_structures.get(function, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ae4bd",
   "metadata": {},
   "source": [
    "`replace_with_false_data_structure` replaces the given `data_structure` with the **false** version of the data structure, and also replaces all **dependent** functions and data structures with their **true versions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc6f4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_false_data_structure(nb, data_structure, false_data_structure):\n",
    "    idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (data_structure))[-1]\n",
    "    if idx == None:\n",
    "        idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "    nb = inject_code(nb, idx, false_data_structure)\n",
    "    nb = remove_initializations(nb, data_structure, start=idx+1)\n",
    "    \n",
    "    for dependent in data_structure_dependencies_functions.get(data_structure, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in data_structure_dependencies_data_structures.get(data_structure, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8aa45",
   "metadata": {},
   "source": [
    "`get_test_text` returns test code that can be readily injected into the notebook. The input should be some code that updates the variable `test_output` and sets its value to be `\"All test cases passed!\"` when the conditions for passing the rubric test are met. This function will place this code inside a wrapper than ensures that it does not crash the student notebook during execution and also makes the output parsable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b561fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_text(qnum, test_code):\n",
    "    test_text = \"\\\"\\\"\\\"grader.check('%s')\\\"\\\"\\\"\\n\\n\" % (qnum)\n",
    "    test_text += \"test_output = '%s results: Test crashed!'\\n\" % (qnum)\n",
    "    test_text += add_try_except(test_code)\n",
    "    test_text += \"\\nprint(test_output)\"\n",
    "    return test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba932dd",
   "metadata": {},
   "source": [
    "`inject_function_logic_check` injects code into the `nb` that detects whether `function` outputs the same as the **true version** of that function (all dependent functions and data structures are also replaced with their **true versions**) on all combinations of inputs from `var_lists`. The comparison between the outputs is performed assuming that the format of the answers is `test_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a0529145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_function_logic_check(nb, function, var_inputs_code, test_format=\"TEXT_FORMAT\"):\n",
    "    for dependent in function_dependencies_functions.get(function, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in function_dependencies_data_structures.get(function, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "        \n",
    "    code = replace_call(true_functions[function], function, \"true_\"+function)\n",
    "    code += \"\\n\\n\" + verify_fn_defn\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    test_code = var_inputs_code + \"\\n\"\n",
    "    test_code += \"test_output = verify_fn(true_%s, %s, var_inputs, '%s')\" % (function, function, test_format)\n",
    "    code = get_test_text(function, test_code)\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc43cf8",
   "metadata": {},
   "source": [
    "`inject_data_structure_check` injects code into the `nb` that detects whether `data_structure` has the same value as the **true version** of that data structure (all dependent functions and data structures are also replaced with their **true versions**). The comparison between the outputs is performed assuming that the format of the answers is `test_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a17b0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_data_structure_check(nb, data_structure, test_format=\"TEXT_FORMAT\"):\n",
    "    for dependent in data_structure_dependencies_functions.get(data_structure, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in data_structure_dependencies_data_structures.get(data_structure, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "        \n",
    "    code = \"import copy\\n%s = copy.deepcopy(%s)\\n\\n\" % (data_structure, data_structure)\n",
    "    code += replace_variable(true_data_structures[data_structure], data_structure, \"true_\"+data_structure)\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    \n",
    "    test_code = \"test_output = '%s results: '\" % (data_structure)\n",
    "    test_code += \"+ public_tests.compare(true_%s, %s, '%s')\" % (data_structure, data_structure, test_format)\n",
    "    code = get_test_text(data_structure, test_code)\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9952fe4c",
   "metadata": {},
   "source": [
    "# END functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95789537",
   "metadata": {},
   "source": [
    "## Random Data Generation\n",
    "\n",
    "Here, functions are defined that can generate **random** data that is in the correct format.\n",
    "\n",
    "**Warning:** This is the most complex function in the file, and is likely to have some bugs in it. So, **verify** this function **carefully**. The following **requirements** for this function **will not** be met by the function generated by GPT, it is **your responsibility** to modify the function so as to meet these requirements. Otherwise, the datasets are unlikely to produce interesting outputs for the project questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bc1ae24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import random\n",
    "\n",
    "def random_data(directory, n=1000):\n",
    "    \"\"\"\n",
    "    Generate random datasets for pokemon_stats.csv and type_effectiveness_stats.csv as described.\n",
    "    The pokemon names are taken from the hidden original pokemon_stats.csv file.\n",
    "    Other values are randomized as per the rules given.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Delete existing files if they exist\n",
    "    pokemon_stats_path = os.path.join(directory, 'pokemon_stats.csv')\n",
    "    type_effectiveness_stats_path = os.path.join(directory, 'type_effectiveness_stats.csv')\n",
    "    \n",
    "    if os.path.exists(pokemon_stats_path):\n",
    "        os.remove(pokemon_stats_path)\n",
    "    \n",
    "    if os.path.exists(type_effectiveness_stats_path):\n",
    "        os.remove(type_effectiveness_stats_path)\n",
    "\n",
    "    # Get Pokemon names from the hidden original file\n",
    "    original_pokemon_stats_path = os.path.join(DIRECTORY, 'hidden', 'original', 'pokemon_stats.csv')\n",
    "    original_type_effectiveness_stats_path = os.path.join(DIRECTORY, 'hidden', 'original', 'type_effectiveness_stats.csv')\n",
    "    with open(original_pokemon_stats_path, 'r', encoding='utf-8') as original_file:\n",
    "        reader = csv.reader(original_file)\n",
    "        next(reader)  # Skip header\n",
    "        pokemon_names = [row[1] for row in reader]  # Gather all Pokemon names\n",
    "    \n",
    "    # Get regions from the hidden original file\n",
    "    regions = set()\n",
    "    with open(original_pokemon_stats_path, 'r', encoding='utf-8') as original_file:\n",
    "        reader = csv.DictReader(original_file)\n",
    "        for row in reader:\n",
    "            regions.add(row['Region'])\n",
    "    regions = list(regions)\n",
    "\n",
    "    # Get types from the type_effectiveness_stats.csv file\n",
    "    types = []\n",
    "    with open(original_type_effectiveness_stats_path, 'r', encoding='utf-8') as types_file:\n",
    "        reader = csv.reader(types_file)\n",
    "        types = next(reader)[1:]  # Skip index column\n",
    "\n",
    "    # Generate pokemon_stats.csv\n",
    "    with open(pokemon_stats_path, 'w', encoding='utf-8', newline='') as ps_file:\n",
    "        writer = csv.writer(ps_file)\n",
    "        headers = ['Name', 'Attack', 'Defense', 'HP', 'Region', 'Sp. Atk', 'Sp. Def', 'Speed', 'Type 1', 'Type 2']\n",
    "        writer.writerow([''] + headers)\n",
    "        for i, name in enumerate(pokemon_names):\n",
    "            type1 =  random.choice(types)\n",
    "            type2 = 'DNE'\n",
    "            type2_choice = random.randint(1, 4)\n",
    "            if type2_choice != 1:\n",
    "                type2 = random.choice(types)\n",
    "                while type2 == type1:\n",
    "                    type2 = random.choice(types)\n",
    "            row = [\n",
    "                name,  # Name\n",
    "                random.randint(1, 256),  # Attack\n",
    "                random.randint(1, 256),  # Defense\n",
    "                random.randint(1, 256),  # HP\n",
    "                random.choice(regions),  # Region\n",
    "                random.randint(1, 256),  # Sp. Atk\n",
    "                random.randint(1, 256),  # Sp. Def\n",
    "                random.randint(1, 256),  # Speed\n",
    "                type1, # Type 1\n",
    "                type2 # Type 2\n",
    "            ]\n",
    "            writer.writerow([i] + row)\n",
    "\n",
    "    # Generate type_effectiveness_stats.csv\n",
    "    with open(type_effectiveness_stats_path, 'w', encoding='utf-8', newline='') as tes_file:\n",
    "        writer = csv.writer(tes_file)\n",
    "        writer.writerow([''] + types)\n",
    "        for type1 in types:\n",
    "            row = [type1] + [random.choice([0.0] + [0.5]*4 + [1.0]*8 + [2.0]*4) for _ in types]\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26c761",
   "metadata": {},
   "source": [
    "## True Functions\n",
    "\n",
    "Here, the **correct** versions of all functions that are defined in the notebook are stored. These functions are compared against the functions in the student notebook to check for their correctness."
   ]
  },
  {
   "cell_type": "raw",
   "id": "73308546",
   "metadata": {},
   "source": [
    "# BEGIN true_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "88ff3b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75c055ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions['damage'] = \"\"\"\n",
    "import project\n",
    "\n",
    "def damage(attacker, defender):\n",
    "    physical_damage = 10 * project.get_attack(attacker) / project.get_defense(defender)\n",
    "    special_damage = 10 * project.get_sp_atk(attacker) / project.get_sp_def(defender)\n",
    "    if physical_damage > special_damage:\n",
    "        return physical_damage\n",
    "    else:\n",
    "        return special_damage\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac5dc7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions['type_bonus'] = \"\"\"\n",
    "import project\n",
    "\n",
    "def type_bonus(attack_type, defender):\n",
    "    defender_type1 = project.get_type1(defender)\n",
    "    defender_type2 = project.get_type2(defender)\n",
    "    bonus = project.get_type_effectiveness(attack_type, defender_type1)\n",
    "    if defender_type2 == \\'DNE\\':\n",
    "        return bonus\n",
    "    else:\n",
    "        bonus = bonus * project.get_type_effectiveness(attack_type, defender_type2)\n",
    "        return bonus\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3ba982f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions['get_num_types'] = \"\"\"\n",
    "import project\n",
    "\n",
    "def get_num_types(pkmn):\n",
    "    if project.get_type2(pkmn) == \\'DNE\\':\n",
    "        return 1\n",
    "    return 2\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "77e3c996",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions['effective_damage'] = \"\"\"\n",
    "import project\n",
    "\n",
    "def effective_damage(attacker, defender):\n",
    "    if get_num_types(attacker) == 1:\n",
    "        return damage(attacker, defender) * type_bonus(project.get_type1(attacker), defender)\n",
    "    else:\n",
    "        bonus1 = type_bonus(project.get_type1(attacker), defender)\n",
    "        bonus2 = type_bonus(project.get_type2(attacker), defender)\n",
    "        return damage(attacker, defender) * max(bonus1, bonus2)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea18de49",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions['num_hits'] = \"\"\"\n",
    "import project\n",
    "import math\n",
    "\n",
    "def num_hits(attacker, defender):\n",
    "    if effective_damage(attacker, defender) == 0:\n",
    "        return \\'infinitely many\\'\n",
    "    return math.ceil(project.get_hp(defender) / effective_damage(attacker, defender))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15b43508",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions['battle'] = \"\"\"\n",
    "import project\n",
    "\n",
    "def battle(pkmn1, pkmn2):\n",
    "    if num_hits(pkmn1, pkmn2) == \\'infinitely many\\':\n",
    "        if num_hits(pkmn2, pkmn1) == \\'infinitely many\\':\n",
    "            return \\'Draw\\'\n",
    "        else:\n",
    "            return pkmn2\n",
    "    elif num_hits(pkmn2, pkmn1) == \\'infinitely many\\':\n",
    "        return pkmn1\n",
    "    elif num_hits(pkmn1, pkmn2) > num_hits(pkmn2, pkmn1):\n",
    "        return pkmn2\n",
    "    elif num_hits(pkmn1, pkmn2) < num_hits(pkmn2, pkmn1):\n",
    "        return pkmn1\n",
    "    elif project.get_speed(pkmn1) > project.get_speed(pkmn2):\n",
    "        return pkmn1\n",
    "    elif project.get_speed(pkmn1) < project.get_speed(pkmn2):\n",
    "        return pkmn2\n",
    "    else:\n",
    "        return \\'Draw\\'\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b923899f",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_functions['friendship_score'] = \"\"\"\n",
    "import project\n",
    "\n",
    "def hidn_get_stat_total(pkmn):\n",
    "    stat_total = project.get_attack(pkmn) + project.get_defense(pkmn)\n",
    "    stat_total += project.get_sp_atk(pkmn) + project.get_sp_def(pkmn)\n",
    "    stat_total += project.get_hp(pkmn) + project.get_speed(pkmn)\n",
    "    return stat_total\n",
    "\n",
    "def friendship_score(pkmn1, pkmn2):\n",
    "    friendship = 0\n",
    "    pkmn1_region = project.get_region(pkmn1)\n",
    "    pkmn2_region = project.get_region(pkmn2)\n",
    "    if pkmn1_region == pkmn2_region:\n",
    "        friendship += 1\n",
    "    if abs(hidn_get_stat_total(pkmn1) - hidn_get_stat_total(pkmn2)) <= 20:\n",
    "        friendship += 1\n",
    "    pkmn1_type1 = project.get_type1(pkmn1)\n",
    "    pkmn1_type2 = project.get_type2(pkmn1)\n",
    "    pkmn2_type1 = project.get_type1(pkmn2)\n",
    "    pkmn2_type2 = project.get_type2(pkmn2)\n",
    "    if pkmn1_type1 == pkmn2_type1:\n",
    "        if pkmn1_type2 != \\'DNE\\' and pkmn1_type2 == pkmn2_type2:\n",
    "            friendship += 3\n",
    "        else:\n",
    "            friendship += 1\n",
    "    elif pkmn1_type2 != \\'DNE\\' and pkmn1_type2 == pkmn2_type2:\n",
    "        friendship += 1\n",
    "    return friendship\"\"\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "d54f997c",
   "metadata": {},
   "source": [
    "# END true_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97473b",
   "metadata": {},
   "source": [
    "## True Data Structures\n",
    "\n",
    "Here, the **correct** versions of all data structures that are defined in the notebook are stored. These data structures are compared against the data structures in the student notebook to check for their correctness."
   ]
  },
  {
   "cell_type": "raw",
   "id": "315ad138",
   "metadata": {},
   "source": [
    "# BEGIN true_data_structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8bfb349c",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_data_structures = {}"
   ]
  },
  {
   "cell_type": "raw",
   "id": "341cf73a",
   "metadata": {},
   "source": [
    "# END true_data_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de10fb3",
   "metadata": {},
   "source": [
    "## Original\n",
    "\n",
    "The original test simply runs the student's notebook as it is (after removing cells with syntax errors, and performing other clean-up). This helps us detect if the student failed any public tests."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe9e4c61",
   "metadata": {},
   "source": [
    "# BEGIN original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57c867a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "\n",
    "results['original'] = parse_nb(run_nb(nb, os.path.join(DIRECTORY, \"hidden\", \"original\", FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecb49232",
   "metadata": {},
   "source": [
    "# END original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec99a",
   "metadata": {},
   "source": [
    "## Hardcode\n",
    "\n",
    "The hardcode tests run the student's notebook on different datasets. However, `public_tests.py` remains unchanged. So, if the answers are hardcoded in the student's notebook, we expect their code to still pass the public tests on all the different datasets. If their code fails any one of the different hardcode datasets, we take that to mean that the answer is not hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a5e9283",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1, 6):\n",
    "    if str(i) in os.listdir(os.path.join(DIRECTORY, \"hidden\", \"hardcode\")):\n",
    "        continue\n",
    "    shutil.copytree(os.path.join(DIRECTORY, \"hidden\", \"original\"), os.path.join(DIRECTORY, \"hidden\", \"hardcode\", str(i)))\n",
    "    \n",
    "for i in range(1, 6):\n",
    "    f = open(os.path.join(DIRECTORY, \"hidden\", \"hardcode\", str(i), 'README.txt'), 'w', encoding='utf-8')\n",
    "    f.write(\"hardcode: %d\" % (i))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dcdcd5a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 done!\n"
     ]
    }
   ],
   "source": [
    "for subdirectory in os.listdir(os.path.join(DIRECTORY, \"hidden\", \"hardcode\")):\n",
    "    path = os.path.join(DIRECTORY, \"hidden\", \"hardcode\", subdirectory)\n",
    "    if not os.path.isdir(path):\n",
    "        continue\n",
    "    good_dataset = False\n",
    "    while not good_dataset:\n",
    "        if os.path.exists(os.path.join(path, FILE)):\n",
    "            nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "        hardcode_results = parse_nb(run_nb(nb, os.path.join(path, FILE)))\n",
    "        good_dataset = True\n",
    "        for qnum in hardcode_results:\n",
    "            if qnum.startswith('q') and hardcode_results[qnum] == 'All test cases passed!':\n",
    "                print(qnum + ' failed!')\n",
    "                good_dataset = False\n",
    "                break\n",
    "        if not good_dataset:\n",
    "            random_data(path)\n",
    "    print(subdirectory + ' done!')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dd5d60f",
   "metadata": {},
   "source": [
    "# BEGIN hardcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "142f91d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "for hardcode in os.listdir(os.path.join(DIRECTORY, \"hidden\", \"hardcode\")):\n",
    "    if hardcode == \".DS_Store\":\n",
    "        continue\n",
    "    nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "    results['hardcode: ' + hardcode] = parse_nb(run_nb(nb, os.path.join(DIRECTORY, \"hidden\", \"hardcode\", hardcode, FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b918781",
   "metadata": {},
   "source": [
    "# END hardcode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365419fe",
   "metadata": {},
   "source": [
    "## Rubric Tests\n",
    "\n",
    "The tests for the rubric points will be defined below. Only the code inside the tags will be executed by `hidden_tests.py`, so the code outside the tags are used for generating the hidden datasets in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8738d",
   "metadata": {},
   "source": [
    "### Instructions for creating rubric tests:\n",
    "\n",
    "Functions inside `hidden_tests.py` can be used to modify the student notebook, before executing and parsing the outputs. It is recommended that before trying to create rubric tests, a user goes through all the functions inside `hidden_tests.py` first. Here is a list of commonly used functions that will be most useful:\n",
    "\n",
    "* **`read_nb`**: `read_nb(file)` **reads** a `file` in the `.ipynb` file format and returns a `nb`.\n",
    "* **`run_nb`**: `run_nb(nb, file)` **executes** `nb` at the location `file` and **writes** the contents back into `file`.\n",
    "* **`parse_nb`**: `parse_nb(nb)` read the contents of a student `nb` and **extracts** all graded questions and answers.\n",
    "* **`truncate_nb`**: `truncate_nb(nb, start, end)` takes in a `nb`, and returns a **sliced** notebook between the cells indexed `start` and `end`.\n",
    "* **`find_all_cell_indices`**: `find_all_cell_indices(nb, cell_type, marker)` returns **all** the indices in `nb` of cell type `cell_type` that **contains** the `marker` in its source.\n",
    "* **`inject_code`**: `inject_code(nb, idx, code)` creates a **new** code cell in `nb` **after** the index `idx` with `code` in it.\n",
    "* **`count_defns`**: `count_defns(nb, func_name)` **counts** the number of times `func_name` is defined in the `nb`.\n",
    "* **`replace_defn`**: `replace_defn(nb, func_name, new_defn)` **replaces** the definition of `func_name` in `nb` with `new_defn`.\n",
    "* **`replace_call`**: `replace_call(text, func_name, new_name)` **replaces** all **calls** and definition **names** to `func_name` with `new_name` in `text`.\n",
    "* **`find_code`**: `find_code(nb, target)` returns the **number** of times that the **text** `target` appears in a code cell in `nb`.\n",
    "* **`replace_code`**: `replace_code(nb, target, new_code, start, end)` **replaces** all instances of the **text** `target` in a code cell between the indices `start` and `end` with the **text** `new_code`.\n",
    "* **`add_try_except`**: `add_try_except(text)` adds a (bare) **try/except block** around any given block of code.\n",
    "* **`detect_restart_and_run_all`**: `detect_restart_and_run_all(nb)` flags if any **non-empty code cell** in `nb` is **not executed**.\n",
    "* **`detect_imports`**: `detect_imports(nb)` returns a list of **all** the **import** statements in the `nb`.\n",
    "* **`detect_ast_objects`**: `detect_ast_objects(nb, objects)` returns a dict of **all** cells in the `nb` with the **ast objects** `objects` in them.\n",
    "* **`get_first_plot`**: `get_first_plot(nb, image_file)` returns the first **image** found in the output of a code cell in `nb`, and also stores it in `image_file` for reference.\n",
    "* **`get_label_plot`**: `get_label_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing just the **label** at the location indicated by `kind` - `\"left\"`, `\"right\"`, `\"top\"`, or `\"bottom\"`.\n",
    "* **`get_without_label_plot`**: `get_without_label_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing everything **except** the **label** at the location indicated by `kind` - `\"left\"`, `\"right\"`, `\"top\"`, or `\"bottom\"`.\n",
    "* **`get_ticks_plot`**: `get_ticks_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing just the **ticks** at the location indicated by `kind` - `\"left\"`, or `\"bottom\"`.\n",
    "* **`get_without_ticks_plot`**: `get_without_ticks_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing everything **except** the **ticks** at the location indicated by `kind` - `\"left\"`, or `\"bottom\"`.\n",
    "* **`get_bounding_box_plot`**: `get_bounding_box_plot(plot)` **crops** the `plot` and returns returns a plot containing just the **bounding box** of the plot.\n",
    "* **`check_text_in_plot`**: `check_text_in_plot(plot, expected_text)` checks if the `expected_text` is in the `plot`, and returns both the **missing** and the **extra** text in the given `plot`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53226213",
   "metadata": {},
   "source": [
    "### damage: function output is incorrect when the `attacker` needs to choose its physical attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9d7858f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'damage: function output is incorrect when the `attacker` needs to choose its physical attack'\n",
    "readme_text = \"\"\"Confirm that your function chooses the correct\n",
    "attack mode by comparing physical and special\n",
    "damage. Ensure you have correctly used the\n",
    "provided functions to compute damage values and\n",
    "are comparing them accurately to return the\n",
    "greater of the two. Review the calculation and\n",
    "conditional statement logic.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3c6ff64",
   "metadata": {},
   "source": [
    "# BEGIN damage: function output is incorrect when the `attacker` needs to choose its physical attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0e955317",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'damage: function output is incorrect when the `attacker` needs to choose its physical attack'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('damage')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attacker in project.__pokemon__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        _physical_damage = 10 * project.get_attack(_attacker) / project.get_defense(_defender)\n",
    "        _special_damage = 10 * project.get_sp_atk(_attacker) / project.get_sp_def(_defender)\n",
    "        if _physical_damage >= _special_damage:\n",
    "            var_inputs.append((_attacker, _defender))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'damage', var_inputs_code, \"TEXT_FORMAT\")\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f8d4fbac",
   "metadata": {},
   "source": [
    "# END damage: function output is incorrect when the `attacker` needs to choose its physical attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d22b369",
   "metadata": {},
   "source": [
    "### damage: function output is incorrect when the `attacker` needs to choose its special attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9cd58ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'damage: function output is incorrect when the `attacker` needs to choose its special attack'\n",
    "readme_text = \"\"\"Confirm that your function chooses the correct\n",
    "attack mode by comparing physical and special\n",
    "damage. Ensure you have correctly used the\n",
    "provided functions to compute damage values and\n",
    "are comparing them accurately to return the\n",
    "greater of the two. Review the calculation and\n",
    "conditional statement logic.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5c4eaa7f",
   "metadata": {},
   "source": [
    "# BEGIN damage: function output is incorrect when the `attacker` needs to choose its special attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "47d44834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'damage: function output is incorrect when the `attacker` needs to choose its special attack'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('damage')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attacker in project.__pokemon__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        _physical_damage = 10 * project.get_attack(_attacker) / project.get_defense(_defender)\n",
    "        _special_damage = 10 * project.get_sp_atk(_attacker) / project.get_sp_def(_defender)\n",
    "        if _physical_damage <= _special_damage:\n",
    "            var_inputs.append((_attacker, _defender))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'damage', var_inputs_code, \"TEXT_FORMAT\")\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5a03db6b",
   "metadata": {},
   "source": [
    "# END damage: function output is incorrect when the `attacker` needs to choose its special attack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a83b80",
   "metadata": {},
   "source": [
    "### q1: correct arguments are not passed to `damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0ae68e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q1: correct arguments are not passed to `damage` function'\n",
    "readme_text = \"\"\"The test checks if you passed the correct\n",
    "arguments to the `damage` function. To fix your\n",
    "code, ensure that the `damage` function must be\n",
    "called with the exact PokÃ©mon names: 'Tinkaton'\n",
    "and 'Arcanine', in that order.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c046ad69",
   "metadata": {},
   "source": [
    "# BEGIN q1: correct arguments are not passed to `damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2d62fedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q1: correct arguments are not passed to `damage` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q1')\")[-1])\n",
    "\n",
    "new_damage = '''\n",
    "def damage(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'damage', new_damage)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ceb97856",
   "metadata": {},
   "source": [
    "# END q1: correct arguments are not passed to `damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b22a153d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd701ea6",
   "metadata": {},
   "source": [
    "### q2: correct arguments are not passed to `damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "500f2e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q2: correct arguments are not passed to `damage` function'\n",
    "readme_text = \"\"\"The test checks if you passed the correct\n",
    "arguments to the `damage` function. To fix your\n",
    "code, ensure that the `damage` function must be\n",
    "called with the exact PokÃ©mon names: 'Lucario'\n",
    "and 'Klawf', in that order.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "daeb6e34",
   "metadata": {},
   "source": [
    "# BEGIN q2: correct arguments are not passed to `damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8bc77471",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q2: correct arguments are not passed to `damage` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q2')\")[-1])\n",
    "\n",
    "new_damage = '''\n",
    "def damage(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'damage', new_damage)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8ecde9df",
   "metadata": {},
   "source": [
    "# END q2: correct arguments are not passed to `damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e9858dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bb522a",
   "metadata": {},
   "source": [
    "### type_bonus: function output is incorrect when the `defender` has only one type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e45a50cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'type_bonus: function output is incorrect when the `defender` has only one type'\n",
    "readme_text = \"\"\"Check if your function correctly handles cases\n",
    "where the `defender` has only one type. Ensure\n",
    "that it correctly uses the `project.get_type1()`\n",
    "and does not apply a second type multiplier if\n",
    "`defender_type2` is `'DNE'`. Review for logic\n",
    "errors that could misinterpret a single-type\n",
    "defender.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "647be533",
   "metadata": {},
   "source": [
    "# BEGIN type_bonus: function output is incorrect when the `defender` has only one type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f01d1122",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'type_bonus: function output is incorrect when the `defender` has only one type'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('type_bonus')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attack_type in project.__effectiveness__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        if project.get_type2(_defender) == \"DNE\":\n",
    "            var_inputs.append((_attack_type, _defender))\n",
    "'''\n",
    "\n",
    "nb = inject_function_logic_check(nb, 'type_bonus', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fbe1d247",
   "metadata": {},
   "source": [
    "# END type_bonus: function output is incorrect when the `defender` has only one type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09b67eff",
   "metadata": {},
   "source": [
    "### type_bonus: function output is incorrect when the `defender` has two types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f30ebf67",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'type_bonus: function output is incorrect when the `defender` has two types'\n",
    "readme_text = \"\"\"Check if your function correctly handles cases\n",
    "where the `defender` has two types. Ensure\n",
    "that it correctly uses the `project.get_type1()`\n",
    "and applies a second type multiplier if\n",
    "`defender_type2` is not `'DNE'`. Review for logic\n",
    "errors that could misinterpret a dual-type\n",
    "defender.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6c2610d3",
   "metadata": {},
   "source": [
    "# BEGIN type_bonus: function output is incorrect when the `defender` has two types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "eab8701b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'type_bonus: function output is incorrect when the `defender` has two types'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('type_bonus')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attack_type in project.__effectiveness__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        if project.get_type2(_defender) != \"DNE\":\n",
    "            var_inputs.append((_attack_type, _defender))\n",
    "'''\n",
    "\n",
    "nb = inject_function_logic_check(nb, 'type_bonus', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "48f503d7",
   "metadata": {},
   "source": [
    "# END type_bonus: function output is incorrect when the `defender` has two types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8edf3ba",
   "metadata": {},
   "source": [
    "### q3: correct arguments are not passed to `type_bonus` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47af332e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q3: correct arguments are not passed to `type_bonus` function'\n",
    "readme_text = \"\"\"Ensure the `type_bonus` function is called with\n",
    "the correct string arguments for the PokÃ©mon type\n",
    "and name. Review the function parameters and the\n",
    "expected arguments.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aa8ab5f1",
   "metadata": {},
   "source": [
    "# BEGIN q3: correct arguments are not passed to `type_bonus` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2c252a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q3: correct arguments are not passed to `type_bonus` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q3')\")[-1])\n",
    "\n",
    "new_type_bonus = '''\n",
    "def type_bonus(attack_type, defender):\n",
    "    return [attack_type, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'type_bonus', new_type_bonus)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c6537cff",
   "metadata": {},
   "source": [
    "# END q3: correct arguments are not passed to `type_bonus` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1e8ce5b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c0fe52",
   "metadata": {},
   "source": [
    "### q4: correct arguments are not passed to `type_bonus` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f90e02d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q4: correct arguments are not passed to `type_bonus` function'\n",
    "readme_text = \"\"\"Ensure the `type_bonus` function is called with\n",
    "the correct string arguments for the PokÃ©mon type\n",
    "and name. Review the function parameters and the\n",
    "expected arguments.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d313f6a2",
   "metadata": {},
   "source": [
    "# BEGIN q4: correct arguments are not passed to `type_bonus` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f5eecb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q4: correct arguments are not passed to `type_bonus` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q4')\")[-1])\n",
    "\n",
    "new_type_bonus = '''\n",
    "def type_bonus(attack_type, defender):\n",
    "    return [attack_type, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'type_bonus', new_type_bonus)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1586ad1d",
   "metadata": {},
   "source": [
    "# END q4: correct arguments are not passed to `type_bonus` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3db3c068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099d47d9",
   "metadata": {},
   "source": [
    "### get_num_types: function logic is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12893bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'get_num_types: function logic is incorrect'\n",
    "readme_text = \"\"\"Check the implementation of `get_num_types` to\n",
    "ensure it properly accounts for all Pokemon types,\n",
    "including 'DNE'. Consider the function's logic and\n",
    "make sure it returns the correct number of types\n",
    "for each input it is given.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b88ca64",
   "metadata": {},
   "source": [
    "# BEGIN get_num_types: function logic is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "05ac05c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'get_num_types: function logic is incorrect'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('get_num_types')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = [(_pkmn,) for _pkmn in list(project.__pokemon__.keys())]\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'get_num_types', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bf85fe42",
   "metadata": {},
   "source": [
    "# END get_num_types: function logic is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36395e48",
   "metadata": {},
   "source": [
    "### effective_damage: `get_num_types` function is not used by `effective_damage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "88dec925",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'effective_damage: `get_num_types` function is not used by `effective_damage`'\n",
    "readme_text = \"\"\"Ensure `get_num_types` is called by\n",
    "`effective_damage` to determine the attacker's\n",
    "number of types. Check if you inadvertently\n",
    "re-implemented its logic instead of calling the\n",
    "function. Consider revisiting function calls and\n",
    "modularity concepts.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ba248aa",
   "metadata": {},
   "source": [
    "# BEGIN effective_damage: `get_num_types` function is not used by `effective_damage`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4b6bc018",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'effective_damage: `get_num_types` function is not used by `effective_damage`'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('effective_damage')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attacker in project.__pokemon__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        var_inputs.append((_attacker, _defender))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'effective_damage', var_inputs_code, \"TEXT_FORMAT\")\n",
    "\n",
    "false_get_num_types = '''\n",
    "def get_num_types(pkmn):\n",
    "    return 1\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'get_num_types', false_get_num_types)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b092b797",
   "metadata": {},
   "source": [
    "# END effective_damage: `get_num_types` function is not used by `effective_damage`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b26e2e",
   "metadata": {},
   "source": [
    "### effective_damage: function output is incorrect when the `attacker` has only one type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "608a594c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'effective_damage: function output is incorrect when the `attacker` has only one type'\n",
    "readme_text = \"\"\"Ensure that your implementation of\n",
    "`effective_damage` correctly handles cases where\n",
    "the attacker has one type. Double-check your use\n",
    "of `get_num_types` within the function. Consider\n",
    "edge cases involving specific inputs not covered\n",
    "by your local tests.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adc81d9a",
   "metadata": {},
   "source": [
    "# BEGIN effective_damage: function output is incorrect when the `attacker` has only one type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ffe41965",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'effective_damage: function output is incorrect when the `attacker` has only one type'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('effective_damage')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attacker in project.__pokemon__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        if project.get_type2(_attacker) == \"DNE\":\n",
    "            var_inputs.append((_attacker, _defender))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'effective_damage', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2ec7491c",
   "metadata": {},
   "source": [
    "# END effective_damage: function output is incorrect when the `attacker` has only one type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "363fcc54",
   "metadata": {},
   "source": [
    "### effective_damage: function output is incorrect when the `attacker` has two types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3dd4548b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'effective_damage: function output is incorrect when the `attacker` has two types'\n",
    "readme_text = \"\"\"Ensure that your implementation of\n",
    "`effective_damage` correctly handles cases where\n",
    "the attacker has two types. Double-check your use\n",
    "of `get_num_types` within the function. Consider\n",
    "edge cases involving specific inputs not covered\n",
    "by your local tests.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b172c274",
   "metadata": {},
   "source": [
    "# BEGIN effective_damage: function output is incorrect when the `attacker` has two types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9ef89f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'effective_damage: function output is incorrect when the `attacker` has two types'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('effective_damage')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attacker in project.__pokemon__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        if project.get_type2(_attacker) != \"DNE\":\n",
    "            var_inputs.append((_attacker, _defender))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'effective_damage', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9b6b532",
   "metadata": {},
   "source": [
    "# END effective_damage: function output is incorrect when the `attacker` has two types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727b4376",
   "metadata": {},
   "source": [
    "### q5: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5aeea737",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q5: correct arguments are not passed to `effective_damage` function'\n",
    "readme_text = \"\"\"Check the arguments passed to `effective_damage`.\n",
    "The function should be called with specific\n",
    "parameters. Review the function's intended inputs.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "532cf05e",
   "metadata": {},
   "source": [
    "# BEGIN q5: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3af88b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q5: correct arguments are not passed to `effective_damage` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q5')\")[-1])\n",
    "\n",
    "new_effective_damage = '''\n",
    "def effective_damage(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'effective_damage', new_effective_damage)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fee1b6b3",
   "metadata": {},
   "source": [
    "# END q5: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f01babb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1093286d",
   "metadata": {},
   "source": [
    "### q6: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5c3ce73e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q6: correct arguments are not passed to `effective_damage` function'\n",
    "readme_text = \"\"\"Check the arguments passed to `effective_damage`.\n",
    "The function should be called with specific\n",
    "parameters. Review the function's intended inputs.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6e42c921",
   "metadata": {},
   "source": [
    "# BEGIN q6: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9c02082f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q6: correct arguments are not passed to `effective_damage` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q6')\")[-1])\n",
    "\n",
    "new_effective_damage = '''\n",
    "def effective_damage(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'effective_damage', new_effective_damage)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6fc46af0",
   "metadata": {},
   "source": [
    "# END q6: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9b022266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0a7206",
   "metadata": {},
   "source": [
    "### q7: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5120ac8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q7: correct arguments are not passed to `effective_damage` function'\n",
    "readme_text = \"\"\"Check the arguments passed to `effective_damage`.\n",
    "The function should be called with specific\n",
    "parameters. Review the function's intended inputs.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6885f7b7",
   "metadata": {},
   "source": [
    "# BEGIN q7: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ab11ab09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q7: correct arguments are not passed to `effective_damage` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q7')\")[-1])\n",
    "\n",
    "new_effective_damage = '''\n",
    "def effective_damage(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'effective_damage', new_effective_damage)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "493d9a1e",
   "metadata": {},
   "source": [
    "# END q7: correct arguments are not passed to `effective_damage` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39771be0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba074a",
   "metadata": {},
   "source": [
    "### num_hits: function output is incorrect when the `attacker` can do non-zero effective damage to the `defender`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8f80fadb",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'num_hits: function output is incorrect when the `attacker` can do non-zero effective damage to the `defender`'\n",
    "readme_text = \"\"\"Ensure your `num_hits` function correctly divides\n",
    "HP by effective damage and rounds up. Use\n",
    "`math.ceil`. Check for any logic errors that might\n",
    "affect the outcome with various inputs.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c973b868",
   "metadata": {},
   "source": [
    "# BEGIN num_hits: function output is incorrect when the `attacker` can do non-zero effective damage to the `defender`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0e314d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'num_hits: function output is incorrect when the `attacker` can do non-zero effective damage to the `defender`'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('num_hits')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attacker in project.__pokemon__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        if effective_damage(_attacker, _defender) != 0:\n",
    "            var_inputs.append((_attacker, _defender))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'num_hits', var_inputs_code, \"TEXT_FORMAT\")\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11ccd46d",
   "metadata": {},
   "source": [
    "# END num_hits: function output is incorrect when the `attacker` can do non-zero effective damage to the `defender`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4345b3",
   "metadata": {},
   "source": [
    "### num_hits: function output is incorrect when the `attacker` cannot do any damage to the `defender`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "188b92d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'num_hits: function output is incorrect when the `attacker` cannot do any damage to the `defender`'\n",
    "readme_text = \"\"\"Ensure `num_hits` correctly handles cases where the\n",
    "`attacker` can do zero effective damage to `defender`.\n",
    "In this case, `num_hits` should return 'infinitely many'. \n",
    "Review handling of division by zero and conditions \n",
    "for effective damage.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "258b9f2f",
   "metadata": {},
   "source": [
    "# BEGIN num_hits: function output is incorrect when the `attacker` cannot do any damage to the `defender`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9aabcce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'num_hits: function output is incorrect when the `attacker` cannot do any damage to the `defender`'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('num_hits')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _attacker in project.__pokemon__:\n",
    "    for _defender in project.__pokemon__:\n",
    "        if effective_damage(_attacker, _defender) == 0:\n",
    "            var_inputs.append((_attacker, _defender))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'num_hits', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "40b1cbca",
   "metadata": {},
   "source": [
    "# END num_hits: function output is incorrect when the `attacker` cannot do any damage to the `defender`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f985f2cd",
   "metadata": {},
   "source": [
    "### q8: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "190cf6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q8: correct arguments are not passed to `num_hits` function'\n",
    "readme_text = \"\"\"The test ensures the `num_hits` function is called\n",
    "with the correct arguments. Verify that the\n",
    "arguments to `num_hits` match the order specified\n",
    "in the function definition and that you are using\n",
    "the correct Pokemon names as parameters.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ded2e67a",
   "metadata": {},
   "source": [
    "# BEGIN q8: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "7fe8e9ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q8: correct arguments are not passed to `num_hits` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q8')\")[-1])\n",
    "\n",
    "new_num_hits = '''\n",
    "def num_hits(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'num_hits', new_num_hits)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eed1cea0",
   "metadata": {},
   "source": [
    "# END q8: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "58fc2261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87b7c41",
   "metadata": {},
   "source": [
    "### q9: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "09fb6518",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q9: correct arguments are not passed to `num_hits` function'\n",
    "readme_text = \"\"\"The test ensures the `num_hits` function is called\n",
    "with the correct arguments. Verify that the\n",
    "arguments to `num_hits` match the order specified\n",
    "in the function definition and that you are using\n",
    "the correct Pokemon names as parameters.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "aab4b61e",
   "metadata": {},
   "source": [
    "# BEGIN q9: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "021d0427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q9: correct arguments are not passed to `num_hits` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q9')\")[-1])\n",
    "\n",
    "new_num_hits = '''\n",
    "def num_hits(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'num_hits', new_num_hits)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8de2a2d9",
   "metadata": {},
   "source": [
    "# END q9: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "9b7df1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62758800",
   "metadata": {},
   "source": [
    "### q10: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c6bdd657",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q10: correct arguments are not passed to `num_hits` function'\n",
    "readme_text = \"\"\"The test ensures the `num_hits` function is called\n",
    "with the correct arguments. Verify that the\n",
    "arguments to `num_hits` match the order specified\n",
    "in the function definition and that you are using\n",
    "the correct Pokemon names as parameters.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e3f3b9d",
   "metadata": {},
   "source": [
    "# BEGIN q10: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "79b7a5f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q10: correct arguments are not passed to `num_hits` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q10')\")[-1])\n",
    "\n",
    "new_num_hits = '''\n",
    "def num_hits(attacker, defender):\n",
    "    return [attacker, defender]\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'num_hits', new_num_hits)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d5aa53d4",
   "metadata": {},
   "source": [
    "# END q10: correct arguments are not passed to `num_hits` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5e68db62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f309ba5",
   "metadata": {},
   "source": [
    "### battle: function output is incorrect when the two Pokemon can do damage to each other and do not take the same number of hits to defeat each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "81412753",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'battle: function output is incorrect when the two Pokemon can do damage to each other and do not take the same number of hits to defeat each other'\n",
    "readme_text = \"\"\"Ensure that your `battle` function correctly\n",
    "compares the number of hits each Pokemon can take\n",
    "using the `num_hits` function. Verify that you are\n",
    "not calling the functions more times than needed,\n",
    "which may change the logic. Check for correct\n",
    "comparison logic for the number of hits and speed,\n",
    "if applicable.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26e590cc",
   "metadata": {},
   "source": [
    "# BEGIN battle: function output is incorrect when the two Pokemon can do damage to each other and do not take the same number of hits to defeat each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ffbe2c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'battle: function output is incorrect when the two Pokemon can do damage to each other and do not take the same number of hits to defeat each other'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('battle')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _pkmn1 in project.__pokemon__:\n",
    "    for _pkmn2 in project.__pokemon__:\n",
    "        if num_hits(_pkmn1, _pkmn2) == 'infinitely many':\n",
    "            continue\n",
    "        elif num_hits(_pkmn2, _pkmn1) == 'infinitely many':\n",
    "            continue\n",
    "        elif num_hits(_pkmn1, _pkmn2) == num_hits(_pkmn2, _pkmn1):\n",
    "            continue\n",
    "        var_inputs.append((_pkmn1, _pkmn2))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'battle', var_inputs_code, \"TEXT_FORMAT\")\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cfc94ddf",
   "metadata": {},
   "source": [
    "# END battle: function output is incorrect when the two Pokemon can do damage to each other and do not take the same number of hits to defeat each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95555eb1",
   "metadata": {},
   "source": [
    "### battle: function output is incorrect when the two Pokemon can do damage to each other but take the same number of hits to defeat each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e785bcff",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'battle: function output is incorrect when the two Pokemon can do damage to each other but take the same number of hits to defeat each other'\n",
    "readme_text = \"\"\"The test checks for correct output when two\n",
    "Pokemon can deal damage but take equal hits to\n",
    "defeat each other. Verify your implementation of\n",
    "`battle` and consider edge cases where the Pokemon\n",
    "have the same number of hits to knock each other\n",
    "out, ensuring your code accounts for speed as a\n",
    "tiebreaker.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5f75dd1f",
   "metadata": {},
   "source": [
    "# BEGIN battle: function output is incorrect when the two Pokemon can do damage to each other but take the same number of hits to defeat each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "f6aed395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'battle: function output is incorrect when the two Pokemon can do damage to each other but take the same number of hits to defeat each other'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('battle')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _pkmn1 in project.__pokemon__:\n",
    "    for _pkmn2 in project.__pokemon__:\n",
    "        if num_hits(_pkmn1, _pkmn2) == 'infinitely many':\n",
    "            continue\n",
    "        elif num_hits(_pkmn2, _pkmn1) == 'infinitely many':\n",
    "            continue\n",
    "        elif num_hits(_pkmn1, _pkmn2) == num_hits(_pkmn2, _pkmn1):\n",
    "            var_inputs.append((_pkmn1, _pkmn2))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'battle', var_inputs_code, \"TEXT_FORMAT\")\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4d24e16e",
   "metadata": {},
   "source": [
    "# END battle: function output is incorrect when the two Pokemon can do damage to each other but take the same number of hits to defeat each other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd91d946",
   "metadata": {},
   "source": [
    "### battle: function output is incorrect when one or more of the Pokemon cannot damage the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "9a226222",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'battle: function output is incorrect when one or more of the Pokemon cannot damage the other'\n",
    "readme_text = \"\"\"The test indicated that your `battle` function\n",
    "might not handle scenarios where one Pokemon\n",
    "cannot damage another correctly. Verify your\n",
    "implementation to ensure that it gracefully\n",
    "addresses cases with 'infinitely many' hits and\n",
    "compares the hits required by each Pokemon\n",
    "accurately, taking into account their speed when\n",
    "necessary.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1b365ba8",
   "metadata": {},
   "source": [
    "# BEGIN battle: function output is incorrect when one or more of the Pokemon cannot damage the other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9cb46a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'battle: function output is incorrect when one or more of the Pokemon cannot damage the other'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, 'code', \"grader.check('battle')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _pkmn1 in project.__pokemon__:\n",
    "    for _pkmn2 in project.__pokemon__:\n",
    "        if num_hits(_pkmn1, _pkmn2) == 'infinitely many':\n",
    "            var_inputs.append((_pkmn1, _pkmn2))\n",
    "        elif num_hits(_pkmn2, _pkmn1) == 'infinitely many':\n",
    "            var_inputs.append((_pkmn1, _pkmn2))        \n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'battle', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b4a74c5",
   "metadata": {},
   "source": [
    "# END battle: function output is incorrect when one or more of the Pokemon cannot damage the other"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d05a34f",
   "metadata": {},
   "source": [
    "### q11: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8b8efab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q11: correct arguments are not passed to `battle` function'\n",
    "readme_text = \"\"\"The test verifies if the `battle` function is\n",
    "called with the correct arguments. Please ensure\n",
    "that the function is defined correctly and\n",
    "receives the expected arguments. Review function\n",
    "definition and argument passing in your code.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "907e7919",
   "metadata": {},
   "source": [
    "# BEGIN q11: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a822b839",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q11: correct arguments are not passed to `battle` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q11')\")[-1])\n",
    "\n",
    "new_battle = '''\n",
    "def battle(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'battle', new_battle)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6123cd0f",
   "metadata": {},
   "source": [
    "# END q11: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "4ee1948d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93682b16",
   "metadata": {},
   "source": [
    "### q12: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d06cb2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q12: correct arguments are not passed to `battle` function'\n",
    "readme_text = \"\"\"The test verifies if the `battle` function is\n",
    "called with the correct arguments. Please ensure\n",
    "that the function is defined correctly and\n",
    "receives the expected arguments. Review function\n",
    "definition and argument passing in your code.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e864131e",
   "metadata": {},
   "source": [
    "# BEGIN q12: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "29751d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q12: correct arguments are not passed to `battle` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q12')\")[-1])\n",
    "\n",
    "new_battle = '''\n",
    "def battle(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'battle', new_battle)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5bdbd91e",
   "metadata": {},
   "source": [
    "# END q12: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5d024fde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b06d4f",
   "metadata": {},
   "source": [
    "### q13: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "c289dfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q13: correct arguments are not passed to `battle` function'\n",
    "readme_text = \"\"\"The test verifies if the `battle` function is\n",
    "called with the correct arguments. Please ensure\n",
    "that the function is defined correctly and\n",
    "receives the expected arguments. Review function\n",
    "definition and argument passing in your code.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f54db838",
   "metadata": {},
   "source": [
    "# BEGIN q13: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "23cd0040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q13: correct arguments are not passed to `battle` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q13')\")[-1])\n",
    "\n",
    "new_battle = '''\n",
    "def battle(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'battle', new_battle)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "db207009",
   "metadata": {},
   "source": [
    "# END q13: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5cd8ea8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0427f79",
   "metadata": {},
   "source": [
    "### q14: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "93e5c1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q14: correct arguments are not passed to `battle` function'\n",
    "readme_text = \"\"\"The test verifies if the `battle` function is\n",
    "called with the correct arguments. Please ensure\n",
    "that the function is defined correctly and\n",
    "receives the expected arguments. Review function\n",
    "definition and argument passing in your code.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "910a038a",
   "metadata": {},
   "source": [
    "# BEGIN q14: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c1a3844a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q14: correct arguments are not passed to `battle` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q14')\")[-1])\n",
    "\n",
    "new_battle = '''\n",
    "def battle(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'battle', new_battle)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "00331903",
   "metadata": {},
   "source": [
    "# END q14: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8fd4814c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04384502",
   "metadata": {},
   "source": [
    "### q15: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "5925f77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q15: correct arguments are not passed to `battle` function'\n",
    "readme_text = \"\"\"The test verifies if the `battle` function is\n",
    "called with the correct arguments. Please ensure\n",
    "that the function is defined correctly and\n",
    "receives the expected arguments. Review function\n",
    "definition and argument passing in your code.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "33501052",
   "metadata": {},
   "source": [
    "# BEGIN q15: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9b21a328",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q15: correct arguments are not passed to `battle` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q15')\")[-1])\n",
    "\n",
    "new_battle = '''\n",
    "def battle(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'battle', new_battle)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7598390",
   "metadata": {},
   "source": [
    "# END q15: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "1df33bfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fa7c4f",
   "metadata": {},
   "source": [
    "### q16: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "97adb11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q16: correct arguments are not passed to `battle` function'\n",
    "readme_text = \"\"\"The test verifies if the `battle` function is\n",
    "called with the correct arguments. Please ensure\n",
    "that the function is defined correctly and\n",
    "receives the expected arguments. Review function\n",
    "definition and argument passing in your code.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f963ec89",
   "metadata": {},
   "source": [
    "# BEGIN q16: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b452d90a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q16: correct arguments are not passed to `battle` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q16')\")[-1])\n",
    "\n",
    "new_battle = '''\n",
    "def battle(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'battle', new_battle)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a0c4ad6a",
   "metadata": {},
   "source": [
    "# END q16: correct arguments are not passed to `battle` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "579d3e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13d8dd",
   "metadata": {},
   "source": [
    "### friendship_score: function output is incorrect when the stat difference of the two Pokemon is exactly 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "222df8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'friendship_score: function output is incorrect when the stat difference of the two Pokemon is exactly 20'\n",
    "readme_text = \"\"\"Ensure the comparison for stat difference accounts\n",
    "for a maximum difference of 20 inclusively. Review\n",
    "conditional checks and the usage of `abs()` in\n",
    "your implementation.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "633c8852",
   "metadata": {},
   "source": [
    "# BEGIN friendship_score: function output is incorrect when the stat difference of the two Pokemon is exactly 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "38ea19e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'friendship_score: function output is incorrect when the stat difference of the two Pokemon is exactly 20'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('friendship_score')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _pkmn1 in project.__pokemon__:\n",
    "    for _pkmn2 in project.__pokemon__:\n",
    "        if abs(hidn_get_stat_total(_pkmn1) - hidn_get_stat_total(_pkmn2)) != 20:\n",
    "            continue\n",
    "        _pkmn1_types = [project.get_type1(_pkmn1), project.get_type2(_pkmn1)]\n",
    "        _pkmn2_types = [project.get_type1(_pkmn2), project.get_type2(_pkmn2)]\n",
    "        if _pkmn1_types[0] in _pkmn2_types:\n",
    "            continue\n",
    "        elif _pkmn2_types[0] in _pkmn1_types:\n",
    "            continue\n",
    "        elif (_pkmn1_types[1] != \"DNE\" and _pkmn1_types[1] in _pkmn2_types):\n",
    "            continue\n",
    "        elif (_pkmn2_types[1] != \"DNE\" and _pkmn2_types[1] in _pkmn1_types):\n",
    "            continue\n",
    "        var_inputs.append((_pkmn1, _pkmn2))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'friendship_score', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f68491dc",
   "metadata": {},
   "source": [
    "# END friendship_score: function output is incorrect when the stat difference of the two Pokemon is exactly 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720f60e3",
   "metadata": {},
   "source": [
    "### friendship_score: function output is incorrect when the two Pokemon have the same types but not necessarily the same corresponding types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "88f63a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'friendship_score: function output is incorrect when the two Pokemon have the same types but not necessarily the same corresponding types'\n",
    "readme_text = \"\"\"Ensure your code strictly checks for corresponding\n",
    "`type1` with `type1` and `type2` with `type2`.\n",
    "Misaligned type comparisons or ignoring the `DNE`\n",
    "case for `type2` can affect the result. Validate\n",
    "comparisons for both type order and equality.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9e88175b",
   "metadata": {},
   "source": [
    "# BEGIN friendship_score: function output is incorrect when the two Pokemon have the same types but not necessarily the same corresponding types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5f89e67b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'friendship_score: function output is incorrect when the two Pokemon have the same types but not necessarily the same corresponding types'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('friendship_score')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _pkmn1 in project.__pokemon__:\n",
    "    for _pkmn2 in project.__pokemon__:\n",
    "        if abs(hidn_get_stat_total(_pkmn1) - hidn_get_stat_total(_pkmn2)) == 20:\n",
    "            continue\n",
    "        elif not {project.get_type1(_pkmn1), project.get_type2(_pkmn1)} == {project.get_type1(_pkmn2), project.get_type2(_pkmn2)}:\n",
    "            continue\n",
    "        elif [project.get_type1(_pkmn1), project.get_type2(_pkmn1)] == [project.get_type1(_pkmn2), project.get_type2(_pkmn2)]:\n",
    "            continue\n",
    "        var_inputs.append((_pkmn1, _pkmn2))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'friendship_score', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "74ad51dc",
   "metadata": {},
   "source": [
    "# END friendship_score: function output is incorrect when the two Pokemon have the same types but not necessarily the same corresponding types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ac24ce",
   "metadata": {},
   "source": [
    "### friendship_score: function logic is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "1fb3efc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'friendship_score: function logic is incorrect'\n",
    "readme_text = \"\"\"Verify that your `friendship_score` function\n",
    "correctly follows all the rules, especially when\n",
    "the stat difference is not exactly 20 and when\n",
    "both types do not match. Ensure proper checks for\n",
    "'DNE' in `type2` and correct point allocation for\n",
    "each rule.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73feac08",
   "metadata": {},
   "source": [
    "# BEGIN friendship_score: function logic is incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "a314ab76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'friendship_score: function logic is incorrect'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('friendship_score')\")[-1])\n",
    "\n",
    "var_inputs_code = '''\n",
    "import project\n",
    "var_inputs = []\n",
    "for _pkmn1 in project.__pokemon__:\n",
    "    for _pkmn2 in project.__pokemon__:\n",
    "        var_inputs.append((_pkmn1, _pkmn2))\n",
    "'''\n",
    "nb = inject_function_logic_check(nb, 'friendship_score', var_inputs_code, 'TEXT_FORMAT')\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))\n",
    "test_output = results[rubric_item][rubric_item.split(\":\")[0]]\n",
    "if test_output != 'All test cases passed!':\n",
    "    comments[rubric_item] += '\\nFAILED TEST CASE: ' + test_output"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a588e56",
   "metadata": {},
   "source": [
    "# END friendship_score: function logic is incorrect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc05e2ab",
   "metadata": {},
   "source": [
    "### q17: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2e31d097",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q17: correct arguments are not passed to `friendship_score` function'\n",
    "readme_text = \"\"\"Check that your `friendship_score` function is\n",
    "defined correctly and accepts two parameters. Make\n",
    "sure to pass the correct string arguments when\n",
    "calling the function.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "dc9c688d",
   "metadata": {},
   "source": [
    "# BEGIN q17: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "8e3755f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q17: correct arguments are not passed to `friendship_score` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q17')\")[-1])\n",
    "\n",
    "false_friendship_score = '''\n",
    "def friendship_score(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'friendship_score', false_friendship_score)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "155850c6",
   "metadata": {},
   "source": [
    "# END q17: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "323d1d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdebae3",
   "metadata": {},
   "source": [
    "### q18: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "4e2ba74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q18: correct arguments are not passed to `friendship_score` function'\n",
    "readme_text = \"\"\"Check that your `friendship_score` function is\n",
    "defined correctly and accepts two parameters. Make\n",
    "sure to pass the correct string arguments when\n",
    "calling the function.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4336a60",
   "metadata": {},
   "source": [
    "# BEGIN q18: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e673bd23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q18: correct arguments are not passed to `friendship_score` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q18')\")[-1])\n",
    "\n",
    "false_friendship_score = '''\n",
    "def friendship_score(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'friendship_score', false_friendship_score)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "adb0695f",
   "metadata": {},
   "source": [
    "# END q18: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "03da6a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa12a33b",
   "metadata": {},
   "source": [
    "### q19: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "7126a52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q19: correct arguments are not passed to `friendship_score` function'\n",
    "readme_text = \"\"\"Check that your `friendship_score` function is\n",
    "defined correctly and accepts two parameters. Make\n",
    "sure to pass the correct string arguments when\n",
    "calling the function.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "524588b9",
   "metadata": {},
   "source": [
    "# BEGIN q19: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "14bb4893",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q19: correct arguments are not passed to `friendship_score` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q19')\")[-1])\n",
    "\n",
    "false_friendship_score = '''\n",
    "def friendship_score(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'friendship_score', false_friendship_score)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "14014747",
   "metadata": {},
   "source": [
    "# END q19: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "179ca1ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2060851f",
   "metadata": {},
   "source": [
    "### q20: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "b09dcad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = 'q20: correct arguments are not passed to `friendship_score` function'\n",
    "readme_text = \"\"\"Check that your `friendship_score` function is\n",
    "defined correctly and accepts two parameters. Make\n",
    "sure to pass the correct string arguments when\n",
    "calling the function.\"\"\"\n",
    "\n",
    "write_readme(readme_text, os.path.join(directories[rubric_item], \"README.txt\"))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a709772e",
   "metadata": {},
   "source": [
    "# BEGIN q20: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "91632f02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "rubric_item = 'q20: correct arguments are not passed to `friendship_score` function'\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('q20')\")[-1])\n",
    "\n",
    "false_friendship_score = '''\n",
    "def friendship_score(pkmn1, pkmn2):\n",
    "    return {pkmn1, pkmn2}\n",
    "'''\n",
    "nb = replace_with_false_function(nb, 'friendship_score', false_friendship_score)\n",
    "\n",
    "results[rubric_item] = parse_nb(run_nb(nb, os.path.join(directories[rubric_item], FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f618bda2",
   "metadata": {},
   "source": [
    "# END q20: correct arguments are not passed to `friendship_score` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "68f5cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "gen_public_tests.gen_public_tests(os.path.join(directories[rubric_item], FILE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101b2d1a",
   "metadata": {},
   "source": [
    "### general_deductions: Did not save the notebook file prior to running the cell containing \"export\". We cannot see your output if you do not save before generating the zip file. This deduction will become stricter for future projects."
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b374358",
   "metadata": {},
   "source": [
    "# BEGIN general_deductions: Did not save the notebook file prior to running the cell containing \"export\". We cannot see your output if you do not save before generating the zip file. This deduction will become stricter for future projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "57a16a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = \"general_deductions: Did not save the notebook file prior to running the cell containing \\\"export\\\". We cannot see your output if you do not save before generating the zip file. This deduction will become stricter for future projects.\"\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('general_deductions')\")[-1])\n",
    "\n",
    "results[rubric_item] = {}\n",
    "results[rubric_item]['general_deductions'] = rubric_item.split(\":\")[1].strip()\n",
    "if detect_restart_and_run_all(nb):\n",
    "    results[rubric_item]['general_deductions'] = \"All test cases passed!\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "f72f2a9f",
   "metadata": {},
   "source": [
    "# END general_deductions: Did not save the notebook file prior to running the cell containing \"export\". We cannot see your output if you do not save before generating the zip file. This deduction will become stricter for future projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d3f8ab",
   "metadata": {},
   "source": [
    "### general_deductions: Functions are defined more than once."
   ]
  },
  {
   "cell_type": "raw",
   "id": "7523c89f",
   "metadata": {},
   "source": [
    "# BEGIN general_deductions: Functions are defined more than once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "78506768",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = \"general_deductions: Functions are defined more than once.\"\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('general_deductions')\")[-1])\n",
    "\n",
    "results[rubric_item] = {}\n",
    "results[rubric_item]['general_deductions'] = 'All test cases passed!'\n",
    "\n",
    "functions = ['damage', 'type_bonus', 'get_num_types', 'effective_damage', 'num_hits', 'battle', 'friendship_score']\n",
    "redefined_functions = []\n",
    "for function in functions:\n",
    "    if count_defns(nb, function) > 1:\n",
    "        redefined_functions.append(function)\n",
    "        \n",
    "if redefined_functions != []:\n",
    "    results[rubric_item]['general_deductions'] = \"following function(s) have multiple definitions:\" + repr(list(redefined_functions))    \n",
    "    comments[rubric_item] = results[rubric_item]['general_deductions']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c31a0143",
   "metadata": {},
   "source": [
    "# END general_deductions: Functions are defined more than once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf4477",
   "metadata": {},
   "source": [
    "### general_deductions: Import statements are not all placed at the top of the notebook."
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9389a85",
   "metadata": {},
   "source": [
    "# BEGIN general_deductions: Import statements are not all placed at the top of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "152c6230",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = \"general_deductions: Import statements are not all placed at the top of the notebook.\"\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, start=find_all_cell_indices(nb, \"markdown\", \"### Function 1: `damage(attack, defender)`\")[0]+1, end=find_all_cell_indices(nb, \"code\", \"grader.check('general_deductions')\")[-1])\n",
    "\n",
    "results[rubric_item] = {}\n",
    "results[rubric_item]['general_deductions'] = 'All test cases passed!'\n",
    "\n",
    "found_imports = detect_imports(nb)\n",
    "if found_imports != []:\n",
    "    results[rubric_item]['general_deductions'] = \"found unexpected import(s):\" + repr(found_imports)\n",
    "    comments[rubric_item] = results[rubric_item]['general_deductions']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec28de00",
   "metadata": {},
   "source": [
    "# END general_deductions: Import statements are not all placed at the top of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9d271e",
   "metadata": {},
   "source": [
    "### general_deductions: Used loops or other material not covered in class yet."
   ]
  },
  {
   "cell_type": "raw",
   "id": "0775a302",
   "metadata": {},
   "source": [
    "# BEGIN general_deductions: Used loops or other material not covered in class yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "5acc8cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rubric_item = \"general_deductions: Used loops or other material not covered in class yet.\"\n",
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "nb = truncate_nb(nb, end=find_all_cell_indices(nb, \"code\", \"grader.check('general_deductions')\")[-1])\n",
    "\n",
    "results[rubric_item] = {}\n",
    "found_bad_objects = detect_ast_objects(nb, [ast.For, ast.While, ast.List, ast.Dict, ast.Set])\n",
    "found_imports = set(detect_imports(nb)) - {\"otter\", \"public_tests\", \"project\", \"math\", \"math.ceil\", \"statistics\"}\n",
    "if found_imports  == set():\n",
    "    if found_bad_objects == {}:\n",
    "        results[rubric_item]['general_deductions'] = \"All test cases passed!\"\n",
    "    else:\n",
    "        results[rubric_item]['general_deductions'] = \"found unexpected statement(s):\\n\" + repr(found_bad_objects)\n",
    "        comments[rubric_item] = results[rubric_item]['general_deductions']\n",
    "else:\n",
    "    results[rubric_item]['general_deductions'] = \"found unexpected import(s):\" + repr(list(found_imports))\n",
    "    comments[rubric_item] = results[rubric_item]['general_deductions']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "73b69ed9",
   "metadata": {},
   "source": [
    "# END general_deductions: Used loops or other material not covered in class yet."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
