{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b1b2de0",
   "metadata": {},
   "source": [
    "# Hidden Tests\n",
    "\n",
    "In this file, the hidden tests for all the rubric points are to be described. The tests for the individual rubric points are enclosed within `# BEGIN <rubric_point>` and `# END <rubric_point>` NBConvert cells. `hidden_tests.py` works by executing the contents of those cells between those two tags for each `<rubric_point>`. In order to initialize variables, `hidden_tests.py` also executes all code within `BEGIN` and `END` tags that appear before the `original` test.\n",
    "\n",
    "Code that is not enclosed within `BEGIN` and `END` tags are not executed by `hidden_tests.py`. They are used for generating the hidden datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8dd714",
   "metadata": {},
   "outputs": [],
   "source": [
    "from hidden_tests import *\n",
    "import otter_tests.gen_public_tests as gen_public_tests\n",
    "import os, csv, json, copy, shutil\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef81058a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIRECTORY = ...\n",
    "FILE = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f614845",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b15ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "deductions = {}\n",
    "rubric = parse_rubric_file(os.path.join(DIRECTORY, \"rubric.md\"))\n",
    "directories = get_directories(rubric)\n",
    "comments = get_all_comments(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe76361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_readme(data, write_path):\n",
    "    \"\"\"write_readme(data, write_path) writes the contents of `data` into the README.txt file `write_path`\"\"\"\n",
    "    f = open(write_path, encoding='utf-8')\n",
    "    rubric_point = f.read().split(\"\\n\")[0].strip(\" \\n\")\n",
    "    f.close()\n",
    "    \n",
    "    f = open(write_path, 'w', encoding='utf-8')\n",
    "    f.write(rubric_point + \"\\n\\n\" + data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb6362",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n",
    "Useful variables that are used by many rubric tests can be stored here. The contents of this tag will be executed before each rubric test, so these variables get initialized before each rubric test."
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9d7477c",
   "metadata": {},
   "source": [
    "# BEGIN variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eafaee",
   "metadata": {},
   "source": [
    "`verify_fn_defn` defines the function `verify_fn` which is used for verifying if the function `expected` and `actual` have the same outputs for all permutations of inputs from `var_lists`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a0f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "verify_fn_defn = \"\"\"\n",
    "def verify_fn(expected, actual, var_inputs, test_format):\n",
    "    for var in var_inputs:\n",
    "        try:\n",
    "            actual_val = actual(*var)\n",
    "        except Exception as e:\n",
    "            output = \"%s results: \" % actual.__name__\n",
    "            output += \"%s error enountered on %s%s\" % (type(e).__name__, actual.__name__, repr(var))\n",
    "            return output\n",
    "        expected_val = expected(*var)\n",
    "        check = public_tests.compare(expected_val, actual_val, test_format)\n",
    "        if check != public_tests.PASS:\n",
    "            output = \"%s results: \" % actual.__name__\n",
    "            output += \"%s%s output: %s\" % (actual.__name__, repr(var), check)\n",
    "            return output\n",
    "    return \"%s results: All test cases passed!\" % actual.__name__\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f689911b",
   "metadata": {},
   "source": [
    "`function_dependencies_functions` stores the previously defined functions that each function definition invokes. This variable is used for rubric points that the logical correctness of functions as well as those that check whether a required function is used. For these rubric points, when we test a particular function, we use `function_dependencies_functions` to ensure that all the functions that it depends on are replaced with logically correct versions. This helps isolate the issue with the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307ebda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_dependencies_functions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7153663c",
   "metadata": {},
   "source": [
    "`function_dependencies_data_structures` stores the previously defined data structures that each function definition invokes. This variable is used for rubric points that the logical correctness of functions as well as those that check whether a required function is used. For these rubric points, when we test a particular function, we use `function_dependencies_data_structures` to ensure that all the data structures that it depends on are replaced with logically correct versions. This helps isolate the issue with the functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935aaa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "function_dependencies_data_structures = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bed966",
   "metadata": {},
   "source": [
    "`data_structure_dependencies_functions` stores the previously defined functions that each data structure definition invokes. This variable is used for rubric points that the logical correctness of functions as well as those that check whether a required data structure is used. For these rubric points, when we test a particular data structure, we use `data_structure_dependencies_functions` to ensure that all the functions that it depends on are replaced with logically correct versions. This helps isolate the issue with the data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e015b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_dependencies_functions = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bdf98",
   "metadata": {},
   "source": [
    "`data_structure_dependencies_data_structures` stores the previously defined data structures that each data structure definition accesses. This variable is used for rubric points that the logical correctness of data structures as well as those that check whether a required data structure is used. For these rubric points, when we test a particular data structure, we use `data_structure_dependencies_data_structures` to ensure that all the data structures that it depends on are replaced with logically correct versions. This helps isolate the issue with the data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db1a4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_structure_dependencies_data_structures = ... "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9fc84609",
   "metadata": {},
   "source": [
    "# END variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc9aa19",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "Useful functions that are used by many rubric tests can be stored here. The contents of this tag will be executed before each rubric test, so these function definitions get initialized before each rubric test."
   ]
  },
  {
   "cell_type": "raw",
   "id": "94d33069",
   "metadata": {},
   "source": [
    "# BEGIN functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdc1d3f",
   "metadata": {},
   "source": [
    "`replace_with_false_function` replaces the given `function` with the **false version** of the function, and also replaces all **dependent** functions and data structures with their **true versions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ad9d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_false_function(nb, function, false_function):\n",
    "    nb = replace_defn(nb, function, false_function)\n",
    "    \n",
    "    for dependent in function_dependencies_functions.get(function, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in function_dependencies_data_structures.get(function, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6ae4bd",
   "metadata": {},
   "source": [
    "`replace_with_false_data_structure` replaces the given `data_structure` with the **false** version of the data structure, and also replaces all **dependent** functions and data structures with their **true versions**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6f4937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_false_data_structure(nb, data_structure, false_data_structure):\n",
    "    idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (data_structure))[-1]\n",
    "    if idx == None:\n",
    "        idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "    nb = inject_code(nb, idx, false_data_structure)\n",
    "    nb = remove_initializations(nb, data_structure, start=idx+1)\n",
    "    \n",
    "    for dependent in data_structure_dependencies_functions.get(data_structure, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in data_structure_dependencies_data_structures.get(data_structure, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b8aa45",
   "metadata": {},
   "source": [
    "`get_test_text` returns test code that can be readily injected into the notebook. The input should be some code that updates the variable `test_output` and sets its value to be `\"All test cases passed!\"` when the conditions for passing the rubric test are met. This function will place this code inside a wrapper than ensures that it does not crash the student notebook during execution and also makes the output parsable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b561fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_text(qnum, test_code):\n",
    "    test_text = \"\\\"\\\"\\\"grader.check('%s')\\\"\\\"\\\"\\n\\n\" % (qnum)\n",
    "    test_text += \"test_output = '%s results: Test crashed!'\\n\" % (qnum)\n",
    "    test_text += add_try_except(test_code)\n",
    "    test_text += \"\\nprint(test_output)\"\n",
    "    return test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ba932dd",
   "metadata": {},
   "source": [
    "`inject_function_logic_check` injects code into the `nb` that detects whether `function` outputs the same as the **true version** of that function (all dependent functions and data structures are also replaced with their **true versions**) on all combinations of inputs from `var_lists`. The comparison between the outputs is performed assuming that the format of the answers is `test_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0529145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_function_logic_check(nb, function, var_inputs_code, test_format=\"TEXT_FORMAT\"):\n",
    "    for dependent in function_dependencies_functions.get(function, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in function_dependencies_data_structures.get(function, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "        \n",
    "    code = replace_call(true_functions[function], function, \"true_\"+function)\n",
    "    code += \"\\n\\n\" + verify_fn_defn\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    test_code = var_inputs_code + \"\\n\"\n",
    "    test_code += \"test_output = verify_fn(true_%s, %s, var_inputs, '%s')\" % (function, function, test_format)\n",
    "    code = get_test_text(function, test_code)\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc43cf8",
   "metadata": {},
   "source": [
    "`inject_data_structure_check` injects code into the `nb` that detects whether `data_structure` has the same value as the **true version** of that data structure (all dependent functions and data structures are also replaced with their **true versions**). The comparison between the outputs is performed assuming that the format of the answers is `test_format`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a17b0fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_data_structure_check(nb, data_structure, test_format=\"TEXT_FORMAT\"):\n",
    "    for dependent in data_structure_dependencies_functions.get(data_structure, []):\n",
    "        nb = replace_defn(nb, dependent, true_functions[dependent])\n",
    "    for dependent in data_structure_dependencies_data_structures.get(data_structure, []):\n",
    "        idx = find_all_cell_indices(nb, \"code\", \"grader.check('%s')\" % (dependent))[-1]\n",
    "        if idx == None:\n",
    "            idx = find_all_cell_indices(nb, \"markdown\", \"**Question 1:**\")[-1]\n",
    "        nb = inject_code(nb, idx, true_data_structures[dependent])\n",
    "        nb = remove_initializations(nb, dependent, start=idx+1)\n",
    "        \n",
    "    code = \"import copy\\n%s = copy.deepcopy(%s)\\n\\n\" % (data_structure, data_structure)\n",
    "    code += replace_variable(true_data_structures[data_structure], data_structure, \"true_\"+data_structure)\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    \n",
    "    test_code = \"test_output = '%s results: '\" % (data_structure)\n",
    "    test_code += \"+ public_tests.compare(true_%s, %s, '%s')\" % (data_structure, data_structure, test_format)\n",
    "    code = get_test_text(data_structure, test_code)\n",
    "    nb = inject_code(nb, len(nb['cells']), code)\n",
    "    return nb"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9952fe4c",
   "metadata": {},
   "source": [
    "# END functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95789537",
   "metadata": {},
   "source": [
    "## Random Data Generation\n",
    "\n",
    "Here, functions are defined that can generate **random** data that is in the correct format.\n",
    "\n",
    "**Warning:** This is the most complex function in the file, and is likely to have some bugs in it. So, **verify** this function **carefully**. The following **requirements** for this function **will not** be met by the function generated by GPT, it is **your responsibility** to modify the function so as to meet these requirements. Otherwise, the datasets are unlikely to produce interesting outputs for the project questions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c26c761",
   "metadata": {},
   "source": [
    "## True Functions\n",
    "\n",
    "Here, the **correct** versions of all functions that are defined in the notebook are stored. These functions are compared against the functions in the student notebook to check for their correctness."
   ]
  },
  {
   "cell_type": "raw",
   "id": "73308546",
   "metadata": {},
   "source": [
    "# BEGIN true_functions"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d54f997c",
   "metadata": {},
   "source": [
    "# END true_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97473b",
   "metadata": {},
   "source": [
    "## True Data Structures\n",
    "\n",
    "Here, the **correct** versions of all data structures that are defined in the notebook are stored. These data structures are compared against the data structures in the student notebook to check for their correctness."
   ]
  },
  {
   "cell_type": "raw",
   "id": "315ad138",
   "metadata": {},
   "source": [
    "# BEGIN true_data_structures"
   ]
  },
  {
   "cell_type": "raw",
   "id": "341cf73a",
   "metadata": {},
   "source": [
    "# END true_data_structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de10fb3",
   "metadata": {},
   "source": [
    "## Original\n",
    "\n",
    "The original test simply runs the student's notebook as it is (after removing cells with syntax errors, and performing other clean-up). This helps us detect if the student failed any public tests."
   ]
  },
  {
   "cell_type": "raw",
   "id": "fe9e4c61",
   "metadata": {},
   "source": [
    "# BEGIN original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c867a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "\n",
    "results['original'] = parse_nb(run_nb(nb, os.path.join(DIRECTORY, \"hidden\", \"original\", FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ecb49232",
   "metadata": {},
   "source": [
    "# END original"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9eec99a",
   "metadata": {},
   "source": [
    "## Hardcode\n",
    "\n",
    "The hardcode tests run the student's notebook on different datasets. However, `public_tests.py` remains unchanged. So, if the answers are hardcoded in the student's notebook, we expect their code to still pass the public tests on all the different datasets. If their code fails any one of the different hardcode datasets, we take that to mean that the answer is not hardcoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdcd5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for subdirectory in os.listdir(os.path.join(DIRECTORY, \"hidden\", \"hardcode\")):\n",
    "    path = os.path.join(DIRECTORY, \"hidden\", \"hardcode\", subdirectory)\n",
    "    good_dataset = False\n",
    "    while not good_dataset:\n",
    "        if os.path.exists(os.path.join(path, FILE)):\n",
    "            nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "        hardcode_results = parse_nb(run_nb(nb, os.path.join(path, FILE)))\n",
    "        good_dataset = True\n",
    "        for qnum in hardcode_results:\n",
    "            if qnum.startswith('q') and hardcode_results[qnum] == 'All test cases passed!':\n",
    "                print(qnum + ' failed!')\n",
    "                good_dataset = False\n",
    "                break\n",
    "        if not good_dataset:\n",
    "            random_data(path, 500)\n",
    "    print(subdirectory + ' done!')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0dd5d60f",
   "metadata": {},
   "source": [
    "# BEGIN hardcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f91d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for hardcode in os.listdir(os.path.join(DIRECTORY, \"hidden\", \"hardcode\")):\n",
    "    nb = clean_nb(read_nb(os.path.join(DIRECTORY, FILE)))\n",
    "    results['hardcode: ' + hardcode] = parse_nb(run_nb(nb, os.path.join(DIRECTORY, \"hidden\", \"hardcode\", hardcode, FILE)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b918781",
   "metadata": {},
   "source": [
    "# END hardcode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365419fe",
   "metadata": {},
   "source": [
    "## Rubric Tests\n",
    "\n",
    "The tests for the rubric points will be defined below. Only the code inside the tags will be executed by `hidden_tests.py`, so the code outside the tags are used for generating the hidden datasets in the first place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a8738d",
   "metadata": {},
   "source": [
    "### Instructions for creating rubric tests:\n",
    "\n",
    "Functions inside `hidden_tests.py` can be used to modify the student notebook, before executing and parsing the outputs. It is recommended that before trying to create rubric tests, a user goes through all the functions inside `hidden_tests.py` first. Here is a list of commonly used functions that will be most useful:\n",
    "\n",
    "* **`read_nb`**: `read_nb(file)` **reads** a `file` in the `.ipynb` file format and returns a `nb`.\n",
    "* **`run_nb`**: `run_nb(nb, file)` **executes** `nb` at the location `file` and **writes** the contents back into `file`.\n",
    "* **`parse_nb`**: `parse_nb(nb)` read the contents of a student `nb` and **extracts** all graded questions and answers.\n",
    "* **`truncate_nb`**: `truncate_nb(nb, start, end)` takes in a `nb`, and returns a **sliced** notebook between the cells indexed `start` and `end`.\n",
    "* **`find_all_cell_indices`**: `find_all_cell_indices(nb, cell_type, marker)` returns **all** the indices in `nb` of cell type `cell_type` that **contains** the `marker` in its source.\n",
    "* **`inject_code`**: `inject_code(nb, idx, code)` creates a **new** code cell in `nb` **after** the index `idx` with `code` in it.\n",
    "* **`count_defns`**: `count_defns(nb, func_name)` **counts** the number of times `func_name` is defined in the `nb`.\n",
    "* **`replace_defn`**: `replace_defn(nb, func_name, new_defn)` **replaces** the definition of `func_name` in `nb` with `new_defn`.\n",
    "* **`replace_call`**: `replace_call(text, func_name, new_name)` **replaces** all **calls** and definition **names** to `func_name` with `new_name` in `text`.\n",
    "* **`find_code`**: `find_code(nb, target)` returns the **number** of times that the **text** `target` appears in a code cell in `nb`.\n",
    "* **`replace_code`**: `replace_code(nb, target, new_code, start, end)` **replaces** all instances of the **text** `target` in a code cell between the indices `start` and `end` with the **text** `new_code`.\n",
    "* **`add_try_except`**: `add_try_except(text)` adds a (bare) **try/except block** around any given block of code.\n",
    "* **`detect_restart_and_run_all`**: `detect_restart_and_run_all(nb)` flags if any **non-empty code cell** in `nb` is **not executed**.\n",
    "* **`detect_imports`**: `detect_imports(nb)` returns a list of **all** the **import** statements in the `nb`.\n",
    "* **`detect_ast_objects`**: `detect_ast_objects(nb, objects)` returns a dict of **all** cells in the `nb` with the **ast objects** `objects` in them.\n",
    "* **`get_first_plot`**: `get_first_plot(nb, image_file)` returns the first **image** found in the output of a code cell in `nb`, and also stores it in `image_file` for reference.\n",
    "* **`get_label_plot`**: `get_label_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing just the **label** at the location indicated by `kind` - `\"left\"`, `\"right\"`, `\"top\"`, or `\"bottom\"`.\n",
    "* **`get_without_label_plot`**: `get_without_label_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing everything **except** the **label** at the location indicated by `kind` - `\"left\"`, `\"right\"`, `\"top\"`, or `\"bottom\"`.\n",
    "* **`get_ticks_plot`**: `get_ticks_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing just the **ticks** at the location indicated by `kind` - `\"left\"`, or `\"bottom\"`.\n",
    "* **`get_without_ticks_plot`**: `get_without_ticks_plot(plot, kind)` **crops** the `plot` and returns returns a plot containing everything **except** the **ticks** at the location indicated by `kind` - `\"left\"`, or `\"bottom\"`.\n",
    "* **`get_bounding_box_plot`**: `get_bounding_box_plot(plot)` **crops** the `plot` and returns returns a plot containing just the **bounding box** of the plot.\n",
    "* **`check_text_in_plot`**: `check_text_in_plot(plot, expected_text)` checks if the `expected_text` is in the `plot`, and returns both the **missing** and the **extra** text in the given `plot`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
